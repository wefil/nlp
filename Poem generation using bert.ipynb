{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-06T15:46:49.680792Z",
     "iopub.status.busy": "2024-04-06T15:46:49.680363Z",
     "iopub.status.idle": "2024-04-06T15:47:06.868094Z",
     "shell.execute_reply": "2024-04-06T15:47:06.867188Z",
     "shell.execute_reply.started": "2024-04-06T15:46:49.680757Z"
    },
    "id": "6rjS5Ovk0AsD",
    "outputId": "66ec1064-3124-4f85-e999-421f5508cea6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "\n",
    "data = pd.read_csv('../input/poetry-foundation-poems/PoetryFoundationData.csv')\n",
    "data = data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-06T15:47:06.870807Z",
     "iopub.status.busy": "2024-04-06T15:47:06.869806Z",
     "iopub.status.idle": "2024-04-06T15:47:06.891808Z",
     "shell.execute_reply": "2024-04-06T15:47:06.890489Z",
     "shell.execute_reply.started": "2024-04-06T15:47:06.870771Z"
    },
    "id": "HoJtndwd5HfC",
    "outputId": "ff38ed6e-9f93-42de-abdd-53c6ecec2b9f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Title</th>\n",
       "      <th>Poem</th>\n",
       "      <th>Poet</th>\n",
       "      <th>Tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>\\r\\r\\n                    Invisible Fish\\r\\r\\n...</td>\n",
       "      <td>\\r\\r\\nInvisible fish swim this ghost ocean now...</td>\n",
       "      <td>Joy Harjo</td>\n",
       "      <td>Living,Time &amp; Brevity,Relationships,Family &amp; A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>\\r\\r\\n                    Don’t Bother the Ear...</td>\n",
       "      <td>\\r\\r\\nDon’t bother the earth spirit who lives ...</td>\n",
       "      <td>Joy Harjo</td>\n",
       "      <td>Religion,The Spiritual,Mythology &amp; Folklore,Fa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>\\r\\r\\n                    [\"Hour in which I co...</td>\n",
       "      <td>\\r\\r\\nHour in which I consider hydrangea, a sa...</td>\n",
       "      <td>Simone White</td>\n",
       "      <td>Living,Parenthood,The Body,The Mind,Nature,Tre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>\\r\\r\\n                    scars\\r\\r\\n         ...</td>\n",
       "      <td>\\r\\r\\nmy father’s body is a map\\r\\r\\na record ...</td>\n",
       "      <td>Truong Tran</td>\n",
       "      <td>The Body,Family &amp; Ancestors</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>\\r\\r\\n                    what remains two\\r\\r...</td>\n",
       "      <td>\\r\\r\\nit has long been forgotten this practice...</td>\n",
       "      <td>Truong Tran</td>\n",
       "      <td>Infancy,Parenthood,The Body</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0                                              Title  \\\n",
       "6            6  \\r\\r\\n                    Invisible Fish\\r\\r\\n...   \n",
       "7            7  \\r\\r\\n                    Don’t Bother the Ear...   \n",
       "9            9  \\r\\r\\n                    [\"Hour in which I co...   \n",
       "16          16  \\r\\r\\n                    scars\\r\\r\\n         ...   \n",
       "17          17  \\r\\r\\n                    what remains two\\r\\r...   \n",
       "\n",
       "                                                 Poem          Poet  \\\n",
       "6   \\r\\r\\nInvisible fish swim this ghost ocean now...     Joy Harjo   \n",
       "7   \\r\\r\\nDon’t bother the earth spirit who lives ...     Joy Harjo   \n",
       "9   \\r\\r\\nHour in which I consider hydrangea, a sa...  Simone White   \n",
       "16  \\r\\r\\nmy father’s body is a map\\r\\r\\na record ...   Truong Tran   \n",
       "17  \\r\\r\\nit has long been forgotten this practice...   Truong Tran   \n",
       "\n",
       "                                                 Tags  \n",
       "6   Living,Time & Brevity,Relationships,Family & A...  \n",
       "7   Religion,The Spiritual,Mythology & Folklore,Fa...  \n",
       "9   Living,Parenthood,The Body,The Mind,Nature,Tre...  \n",
       "16                        The Body,Family & Ancestors  \n",
       "17                        Infancy,Parenthood,The Body  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-06T15:47:06.895605Z",
     "iopub.status.busy": "2024-04-06T15:47:06.895206Z",
     "iopub.status.idle": "2024-04-06T15:47:06.901677Z",
     "shell.execute_reply": "2024-04-06T15:47:06.900538Z",
     "shell.execute_reply.started": "2024-04-06T15:47:06.895572Z"
    },
    "id": "jjDvSLiSt6jq"
   },
   "outputs": [],
   "source": [
    "data = data.head(10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iHKh2yEEz8G4"
   },
   "source": [
    "\n",
    "## **Preprocessing steps**\n",
    "\n",
    "Lowercasing: Converts all text to lowercase, ensuring uniformity.\n",
    "\n",
    "Punctuation Removal: Eliminates non-alphanumeric characters and whitespace, such as punctuation marks, to focus on the words.\n",
    "\n",
    "Special Character Removal: Gets rid of specific special characters, like newline characters, for a cleaner text.\n",
    "\n",
    "Tokenization: Splits the text into individual words or tokens, facilitating further analysis.\n",
    "\n",
    "Stopword Removal: Filters out common words (stopwords) that usually do not contribute much to the meaning of the text.\n",
    "\n",
    "Lemmatization: Reduces words to their base or root form, aiding in standardizing different forms of words.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-06T15:47:06.905612Z",
     "iopub.status.busy": "2024-04-06T15:47:06.904866Z",
     "iopub.status.idle": "2024-04-06T15:47:06.913052Z",
     "shell.execute_reply": "2024-04-06T15:47:06.911616Z",
     "shell.execute_reply.started": "2024-04-06T15:47:06.905571Z"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-06T15:47:06.915322Z",
     "iopub.status.busy": "2024-04-06T15:47:06.914859Z",
     "iopub.status.idle": "2024-04-06T15:47:08.369876Z",
     "shell.execute_reply": "2024-04-06T15:47:08.368149Z",
     "shell.execute_reply.started": "2024-04-06T15:47:06.915284Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  /usr/share/nltk_data/corpora/wordnet.zip\n",
      "   creating: /usr/share/nltk_data/corpora/wordnet/\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/lexnames  \n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/data.verb  \n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/index.adv  \n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/adv.exc  \n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/index.verb  \n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/cntlist.rev  \n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/data.adj  \n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/index.adj  \n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/LICENSE  \n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/citation.bib  \n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/noun.exc  \n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/verb.exc  \n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/README  \n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/index.sense  \n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/data.noun  \n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/data.adv  \n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/index.noun  \n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/adj.exc  \n"
     ]
    }
   ],
   "source": [
    "!unzip /usr/share/nltk_data/corpora/wordnet.zip -d /usr/share/nltk_data/corpora/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-06T15:47:08.374383Z",
     "iopub.status.busy": "2024-04-06T15:47:08.373534Z",
     "iopub.status.idle": "2024-04-06T15:47:10.924317Z",
     "shell.execute_reply": "2024-04-06T15:47:10.923097Z",
     "shell.execute_reply.started": "2024-04-06T15:47:08.374326Z"
    },
    "id": "Xicq4epi3rcr",
    "outputId": "8d1bb977-6cfc-4bef-ad16-852dbbda1abf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example sentence special character stopword\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet \n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "    # Special characters\n",
    "    text = re.sub(r'[\\r\\n]', '', text)\n",
    "\n",
    "    # Token\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "\n",
    "    # Lemmatize\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
    "\n",
    "    return ' '.join(lemmatized_tokens)\n",
    "\n",
    "text_to_preprocess = \"This is an example sentence! It has some special characters *&^%$# and stopword.\"\n",
    "processed_text = preprocess_text(text_to_preprocess)\n",
    "print(processed_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-06T15:47:10.925879Z",
     "iopub.status.busy": "2024-04-06T15:47:10.925563Z",
     "iopub.status.idle": "2024-04-06T15:47:10.934537Z",
     "shell.execute_reply": "2024-04-06T15:47:10.933492Z",
     "shell.execute_reply.started": "2024-04-06T15:47:10.925850Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import re\\nfrom nltk.tokenize import word_tokenize\\nfrom nltk.corpus import stopwords\\nfrom nltk.stem import PorterStemmer\\n\\ndef preprocess_text(text):\\n    # Lowercase\\n    text = text.lower()\\n\\n    # Punctuation\\n    text = re.sub(r\\'[^\\\\w\\\\s]\\', \\'\\', text)\\n\\n    # Special characters\\n    text = re.sub(r\\'[\\r\\n]\\', \\'\\', text)\\n\\n    # Tokenization\\n    tokens = word_tokenize(text)\\n\\n    # Stopwords\\n    stop_words = set(stopwords.words(\\'english\\'))\\n    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\\n\\n    # Stemming\\n    stemmer = PorterStemmer()\\n    stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]\\n\\n    return \\' \\'.join(stemmed_tokens)\\n\\ntext_to_preprocess = \"This is an example sentence! It has some special characters *&^%$# and stopword.\"\\nprocessed_text = preprocess_text(text_to_preprocess)\\nprint(processed_text)'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "    # Special characters\n",
    "    text = re.sub(r'[\\r\\n]', '', text)\n",
    "\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "\n",
    "    # Stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]\n",
    "\n",
    "    return ' '.join(stemmed_tokens)\n",
    "\n",
    "text_to_preprocess = \"This is an example sentence! It has some special characters *&^%$# and stopword.\"\n",
    "processed_text = preprocess_text(text_to_preprocess)\n",
    "print(processed_text)'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-06T15:47:10.936272Z",
     "iopub.status.busy": "2024-04-06T15:47:10.935949Z",
     "iopub.status.idle": "2024-04-06T15:47:51.584741Z",
     "shell.execute_reply": "2024-04-06T15:47:51.583003Z",
     "shell.execute_reply.started": "2024-04-06T15:47:10.936245Z"
    },
    "id": "R8UR5qPH3dBL"
   },
   "outputs": [],
   "source": [
    "data['Title'] = data['Title'].apply(preprocess_text)\n",
    "data['Poem'] = data['Poem'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4iC3130z1s5f"
   },
   "source": [
    "## **Embedding Techniques**\n",
    "\n",
    "1. TF-IDF is used for document representation and importance weighting.\n",
    "2. Word2Vec is employed for generating word embeddings and capturing semantic relationships between words.\n",
    "3. CBOW predicts a target word based on its context, useful for tasks that involve understanding the meaning of words in a given context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Se4gaORN36Su"
   },
   "source": [
    "TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-06T15:47:51.587331Z",
     "iopub.status.busy": "2024-04-06T15:47:51.586950Z",
     "iopub.status.idle": "2024-04-06T15:47:54.759714Z",
     "shell.execute_reply": "2024-04-06T15:47:54.758431Z",
     "shell.execute_reply.started": "2024-04-06T15:47:51.587298Z"
    },
    "id": "BHsG71yE37wV"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "titles_corpus = data['Title']\n",
    "poems_corpus = data['Poem']\n",
    "\n",
    "tfidf_vectorizer_titles = TfidfVectorizer()\n",
    "tfidf_matrix_titles = tfidf_vectorizer_titles.fit_transform(titles_corpus)\n",
    "\n",
    "tfidf_vectorizer_poems = TfidfVectorizer()\n",
    "tfidf_matrix_poems = tfidf_vectorizer_poems.fit_transform(poems_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-06T15:47:54.764249Z",
     "iopub.status.busy": "2024-04-06T15:47:54.763806Z",
     "iopub.status.idle": "2024-04-06T15:47:59.717726Z",
     "shell.execute_reply": "2024-04-06T15:47:59.716503Z",
     "shell.execute_reply.started": "2024-04-06T15:47:54.764215Z"
    },
    "id": "ggv87nuM4Bxh"
   },
   "outputs": [],
   "source": [
    "tfidf_vectors_titles = tfidf_matrix_titles.toarray()\n",
    "tfidf_vectors_poems = tfidf_matrix_poems.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-06T15:47:59.719160Z",
     "iopub.status.busy": "2024-04-06T15:47:59.718847Z",
     "iopub.status.idle": "2024-04-06T15:47:59.727709Z",
     "shell.execute_reply": "2024-04-06T15:47:59.726518Z",
     "shell.execute_reply.started": "2024-04-06T15:47:59.719133Z"
    },
    "id": "_cS3wCS04wGv",
    "outputId": "37e51e31-25ae-41a3-cc31-6b2212039877"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectors_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-06T15:47:59.729504Z",
     "iopub.status.busy": "2024-04-06T15:47:59.729145Z",
     "iopub.status.idle": "2024-04-06T15:47:59.741302Z",
     "shell.execute_reply": "2024-04-06T15:47:59.740009Z",
     "shell.execute_reply.started": "2024-04-06T15:47:59.729448Z"
    },
    "id": "q_J8A0Tw4xoc",
    "outputId": "5ee70913-8b48-452d-99de-bb475e8b3140"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectors_poems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yMpUMZwU3naj"
   },
   "source": [
    "Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "saHWrgJ43Xcf"
   },
   "source": [
    "CBOW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YlurondI3F-o"
   },
   "source": [
    "Visual Representation using word cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-06T15:47:59.745054Z",
     "iopub.status.busy": "2024-04-06T15:47:59.744710Z",
     "iopub.status.idle": "2024-04-06T15:47:59.774494Z",
     "shell.execute_reply": "2024-04-06T15:47:59.773078Z",
     "shell.execute_reply.started": "2024-04-06T15:47:59.745024Z"
    },
    "id": "dl0R0524K75Y",
    "outputId": "e1a47118-ba52-4e0f-fe1a-677099261ccd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 10000 entries, 6 to 10801\n",
      "Data columns (total 5 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   Unnamed: 0  10000 non-null  int64 \n",
      " 1   Title       10000 non-null  object\n",
      " 2   Poem        10000 non-null  object\n",
      " 3   Poet        10000 non-null  object\n",
      " 4   Tags        10000 non-null  object\n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 468.8+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-06T15:47:59.776661Z",
     "iopub.status.busy": "2024-04-06T15:47:59.775966Z",
     "iopub.status.idle": "2024-04-06T15:48:31.727249Z",
     "shell.execute_reply": "2024-04-06T15:48:31.726063Z",
     "shell.execute_reply.started": "2024-04-06T15:47:59.776626Z"
    },
    "id": "D57btcm5LcQx"
   },
   "outputs": [],
   "source": [
    "subset_data = data\n",
    "\n",
    "\n",
    "subset_data['Title'] = subset_data['Title'].apply(preprocess_text)\n",
    "subset_data['Poem'] = subset_data['Poem'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-06T15:48:31.729063Z",
     "iopub.status.busy": "2024-04-06T15:48:31.728701Z",
     "iopub.status.idle": "2024-04-06T18:04:46.761713Z",
     "shell.execute_reply": "2024-04-06T18:04:46.759875Z",
     "shell.execute_reply.started": "2024-04-06T15:48:31.729031Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a99e5c2ed37e432bb4fa2ed0c9dc9e78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e59287cd02a4d68b6ddbff784c4b881",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1fe0caf9695457a89e4eb3318e3bbd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6815204fb82f4b6c9baa71678b403764",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a0b51962de84282894ed83231357eaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (620 > 512). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 1361610.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Poem based on the given title:\n",
      "poem\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "class BertSequenceVectorizer:\n",
    "    def __init__(self):\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.model_name = 'bert-base-uncased'\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(self.model_name)\n",
    "        self.bert_model = BertModel.from_pretrained(self.model_name).to(self.device)\n",
    "        self.max_len = 128\n",
    "\n",
    "    def vectorize(self, sentence: str) -> np.array:\n",
    "        inp = self.tokenizer.encode(sentence, add_special_tokens=True)\n",
    "        len_inp = len(inp)\n",
    "\n",
    "        if len_inp >= self.max_len:\n",
    "            inputs = inp[:self.max_len]\n",
    "            masks = [1] * self.max_len\n",
    "        else:\n",
    "            inputs = inp + [0] * (self.max_len - len_inp)\n",
    "            masks = [1] * len_inp + [0] * (self.max_len - len_inp)\n",
    "\n",
    "        inputs_tensor = torch.tensor([inputs], dtype=torch.long).to(self.device)\n",
    "        masks_tensor = torch.tensor([masks], dtype=torch.long).to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            bert_out = self.bert_model(inputs_tensor, masks_tensor)\n",
    "            seq_out, pooled_out = bert_out.last_hidden_state, bert_out.pooler_output\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            return seq_out[0][0].cpu().detach().numpy()\n",
    "        else:\n",
    "            return seq_out[0][0].detach().numpy()\n",
    "\n",
    "BSV = BertSequenceVectorizer()\n",
    "\n",
    "# Assuming 'data' is your DataFrame containing poems and titles\n",
    "data['Poem_bert'] = tqdm(data['Poem'].apply(lambda x: BSV.vectorize(x)))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['Poem_bert'], data['Title'], test_size=0.2, random_state=42)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "\n",
    "logistic_model = LogisticRegression(max_iter=1000)\n",
    "logistic_model.fit(list(X_train), y_train_encoded)\n",
    "\n",
    "def generate_poem(title):\n",
    "    vectorized_poem = BSV.vectorize(title)\n",
    "    predicted_title_encoded = logistic_model.predict([vectorized_poem])[0]\n",
    "    predicted_title = label_encoder.inverse_transform([predicted_title_encoded])[0]\n",
    "    return predicted_title\n",
    "\n",
    "# Example usage:\n",
    "given_title = \"Brian If you knew who I was now When I knew who you were then Would you forgive me before We ever became them?\"\n",
    "predicted_poem = generate_poem(given_title)\n",
    "print(\"Predicted Poem based on the given title:\")\n",
    "print(predicted_poem)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-06T18:04:46.768403Z",
     "iopub.status.busy": "2024-04-06T18:04:46.766566Z",
     "iopub.status.idle": "2024-04-06T18:04:47.097179Z",
     "shell.execute_reply": "2024-04-06T18:04:47.093226Z",
     "shell.execute_reply.started": "2024-04-06T18:04:46.768357Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Poem based on the given title:\n",
      "grain field\n"
     ]
    }
   ],
   "source": [
    "given_title = \"Oranges\"\n",
    "predicted_poem = generate_poem(given_title)\n",
    "print(\"Predicted Poem based on the given title:\")\n",
    "print(predicted_poem)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Poem based on the given title:\n",
      "white hunter\n"
     ]
    }
   ],
   "source": [
    "given_title = \"Haunted\"\n",
    "predicted_poem = generate_poem(given_title)\n",
    "print(\"Predicted Poem based on the given title:\")\n",
    "print(predicted_poem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-06T18:04:47.446588Z",
     "iopub.status.busy": "2024-04-06T18:04:47.442574Z",
     "iopub.status.idle": "2024-04-06T18:04:47.784597Z",
     "shell.execute_reply": "2024-04-06T18:04:47.782988Z",
     "shell.execute_reply.started": "2024-04-06T18:04:47.446524Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Poem based on the given title:\n",
      "lord\n"
     ]
    }
   ],
   "source": [
    "given_title = \"Forever and Always\"\n",
    "predicted_poem = generate_poem(given_title)\n",
    "print(\"Predicted Poem based on the given title:\")\n",
    "print(predicted_poem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-06T18:37:51.193990Z",
     "iopub.status.busy": "2024-04-06T18:37:51.193554Z",
     "iopub.status.idle": "2024-04-06T18:37:51.475908Z",
     "shell.execute_reply": "2024-04-06T18:37:51.474198Z",
     "shell.execute_reply.started": "2024-04-06T18:37:51.193960Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Poem based on the given title:\n",
      "useless useless\n"
     ]
    }
   ],
   "source": [
    "given_title = \"Ocean\"\n",
    "predicted_poem = generate_poem(given_title)\n",
    "print(\"Predicted Poem based on the given title:\")\n",
    "print(predicted_poem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-06T18:38:05.842925Z",
     "iopub.status.busy": "2024-04-06T18:38:05.842494Z",
     "iopub.status.idle": "2024-04-06T18:38:06.106769Z",
     "shell.execute_reply": "2024-04-06T18:38:06.104999Z",
     "shell.execute_reply.started": "2024-04-06T18:38:05.842890Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Poem based on the given title:\n",
      "snow melting\n"
     ]
    }
   ],
   "source": [
    "given_title = \"Summer time\"\n",
    "predicted_poem = generate_poem(given_title)\n",
    "print(\"Predicted Poem based on the given title:\")\n",
    "print(predicted_poem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-06T18:38:20.105725Z",
     "iopub.status.busy": "2024-04-06T18:38:20.104534Z",
     "iopub.status.idle": "2024-04-06T18:38:20.363693Z",
     "shell.execute_reply": "2024-04-06T18:38:20.362221Z",
     "shell.execute_reply.started": "2024-04-06T18:38:20.105683Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Poem based on the given title:\n",
      "hotel\n"
     ]
    }
   ],
   "source": [
    "given_title = \"Road not taken\"\n",
    "predicted_poem = generate_poem(given_title)\n",
    "print(\"Predicted Poem based on the given title:\")\n",
    "print(predicted_poem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-06T18:38:29.564104Z",
     "iopub.status.busy": "2024-04-06T18:38:29.563389Z",
     "iopub.status.idle": "2024-04-06T18:38:29.829739Z",
     "shell.execute_reply": "2024-04-06T18:38:29.828867Z",
     "shell.execute_reply.started": "2024-04-06T18:38:29.564061Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Poem based on the given title:\n",
      "still life 1\n"
     ]
    }
   ],
   "source": [
    "given_title = \"Cats\"\n",
    "predicted_poem = generate_poem(given_title)\n",
    "print(\"Predicted Poem based on the given title:\")\n",
    "print(predicted_poem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-06T18:38:56.133591Z",
     "iopub.status.busy": "2024-04-06T18:38:56.133158Z",
     "iopub.status.idle": "2024-04-06T18:38:56.396134Z",
     "shell.execute_reply": "2024-04-06T18:38:56.394468Z",
     "shell.execute_reply.started": "2024-04-06T18:38:56.133553Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Poem based on the given title:\n",
      "earth shake\n"
     ]
    }
   ],
   "source": [
    "given_title = \"Earth\"\n",
    "predicted_poem = generate_poem(given_title)\n",
    "print(\"Predicted Poem based on the given title:\")\n",
    "print(predicted_poem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-06T18:39:13.602652Z",
     "iopub.status.busy": "2024-04-06T18:39:13.601858Z",
     "iopub.status.idle": "2024-04-06T18:39:13.866149Z",
     "shell.execute_reply": "2024-04-06T18:39:13.864483Z",
     "shell.execute_reply.started": "2024-04-06T18:39:13.602612Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Poem based on the given title:\n",
      "amidwives two portrait\n"
     ]
    }
   ],
   "source": [
    "given_title = \"Life\"\n",
    "predicted_poem = generate_poem(given_title)\n",
    "print(\"Predicted Poem based on the given title:\")\n",
    "print(predicted_poem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-06T18:39:26.324082Z",
     "iopub.status.busy": "2024-04-06T18:39:26.323640Z",
     "iopub.status.idle": "2024-04-06T18:39:26.621761Z",
     "shell.execute_reply": "2024-04-06T18:39:26.620318Z",
     "shell.execute_reply.started": "2024-04-06T18:39:26.324047Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Poem based on the given title:\n",
      "still life 1\n"
     ]
    }
   ],
   "source": [
    "given_title = \"Mirror\"\n",
    "predicted_poem = generate_poem(given_title)\n",
    "print(\"Predicted Poem based on the given title:\")\n",
    "print(predicted_poem)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 236282,
     "sourceId": 502516,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30673,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

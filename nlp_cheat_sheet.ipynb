{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### **Tweet Preprocessing**"
      ],
      "metadata": {
        "id": "eVvvfl1b1p6a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DuJGP9UBlhKw",
        "outputId": "b4c62729-e00c-4b8d-db63-7fecdfce976d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['jsut', 'poseted', 'someting', 'im', 'scream', 'u']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "def text_preprocessing(text):\n",
        "    # Remove Leading Blank Spaces\n",
        "    text = text.strip()\n",
        "\n",
        "    # Lower Case\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove URLS\n",
        "    url_pattern = re.compile(r\"https?://\\S+|www\\.\\S+\")\n",
        "    text = re.sub(url_pattern, \"\", text)\n",
        "\n",
        "    # Remove UserName\n",
        "    username_pattern = re.compile(r\"@\\w+\")\n",
        "    text = re.sub(username_pattern, \"\", text)\n",
        "\n",
        "    # Remove Hashtags\n",
        "    hashtag_pattern = re.compile(r\"#\\w+\")\n",
        "    text = re.sub(hashtag_pattern, \"\", text)\n",
        "\n",
        "    # Character normalization // todaaaaay -> today\n",
        "    text = re.sub(r\"([a-zA-Z])\\1{2,}\", r'\\1', text)\n",
        "\n",
        "    # Remove Special Characters\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', \"\", text)\n",
        "\n",
        "    # Word Tokenizer\n",
        "    text = word_tokenize(text)\n",
        "\n",
        "    # Remove Stop Words\n",
        "    stop_words = set(stopwords.words(\"english\"))\n",
        "    text = [word for word in text if word not in stop_words]\n",
        "\n",
        "    # Lemmatization\n",
        "    def get_pos(word):\n",
        "        tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "        tag_dict = {\"N\": \"n\", \"V\": \"v\", \"R\": \"r\", \"J\": \"a\"}\n",
        "        return tag_dict.get(tag, \"n\")\n",
        "\n",
        "    lemma = WordNetLemmatizer()\n",
        "    text = [lemma.lemmatize(word, pos=get_pos(word)) for word in text]\n",
        "\n",
        "    return text\n",
        "\n",
        "# Example usage:\n",
        "sample_text = \"@Dia jsut poseted someting (*)@(*$)(*)#@)!!! Im screaming ^U^\"\n",
        "processed_text = text_preprocessing(sample_text)\n",
        "print(processed_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install emoji"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d5PPPfo71JZ5",
        "outputId": "ff103ee8-2afc-46d0-9031-2351e7d3f759"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting emoji\n",
            "  Downloading emoji-2.11.0-py2.py3-none-any.whl (433 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/433.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/433.8 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m368.6/433.8 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m433.8/433.8 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: emoji\n",
            "Successfully installed emoji-2.11.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "import emoji\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "tokenizer = TweetTokenizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_tweet(tweet):\n",
        "    tokens = tokenizer.tokenize(tweet)\n",
        "    tokens = [token.lower() for token in tokens]\n",
        "    tokens = [re.sub(r'https?://\\S+|www\\.\\S+', '', token) for token in tokens]\n",
        "    tokens = [re.sub(r'\\W', ' ', token) for token in tokens if token.isalnum()]\n",
        "    tokens = [token for token in tokens if token not in stop_words]\n",
        "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "    tokens = [emoji.demojize(token) for token in tokens]\n",
        "    contractions = {\n",
        "        \"can't\": \"cannot\",\n",
        "        \"won't\": \"will not\",\n",
        "    }\n",
        "    tokens = [contractions[token] if token in contractions else token for token in tokens]\n",
        "    tokens = [token for token in tokens if not token.startswith('@')]\n",
        "    processed_tweet = ' '.join(tokens)\n",
        "\n",
        "    return processed_tweet\n",
        "\n",
        "preprocess_tweet(\"@Dia jsut poseted someting (*)@(*$)(*)#@)!!! Im screaming ^U^😃\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "sehNVAAo1Cox",
        "outputId": "dc684d96-a420-4b08-cee9-bc71dbe41395"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'jsut poseted someting im screaming u'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "8mMi87qboRSA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Packages**"
      ],
      "metadata": {
        "id": "tRcaqFvrog8n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import emoji\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D\n",
        "from gensim.models.word2vec import Word2Vec\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import *\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.layers import SimpleRNN\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding, Attention\n",
        "\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, LSTM, Dense\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "tzZCWSKxqvZq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "id": "ard4ldwN0E4w"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
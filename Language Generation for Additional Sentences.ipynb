{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "df5a894d",
      "metadata": {
        "id": "df5a894d"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "e73c1d05",
      "metadata": {
        "id": "e73c1d05",
        "outputId": "7e5139e8-edfc-4b2c-af50-76eb2c6c6384",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "             author claps  reading_time  \\\n",
              "0        Justin Lee  8.3K            11   \n",
              "1       Conor Dewey  1.4K             7   \n",
              "2  William Koehrsen  2.8K            11   \n",
              "3      Gant Laborde  1.3K             7   \n",
              "4  Emmanuel Ameisen   935            11   \n",
              "\n",
              "                                                link  \\\n",
              "0  https://medium.com/swlh/chatbots-were-the-next...   \n",
              "1  https://towardsdatascience.com/python-for-data...   \n",
              "2  https://towardsdatascience.com/automated-featu...   \n",
              "3  https://medium.freecodecamp.org/machine-learni...   \n",
              "4  https://blog.insightdatascience.com/reinforcem...   \n",
              "\n",
              "                                               title  \\\n",
              "0  Chatbots were the next big thing: what happene...   \n",
              "1  Python for Data Science: 8 Concepts You May Ha...   \n",
              "2  Automated Feature Engineering in Python â To...   \n",
              "3  Machine Learning: how to go from Zero to Hero ...   \n",
              "4  Reinforcement Learning from scratch â Insigh...   \n",
              "\n",
              "                                                text  \n",
              "0  Oh, how the headlines blared:\\nChatbots were T...  \n",
              "1  If youâve ever found yourself looking up the...  \n",
              "2  Machine learning is increasingly moving from h...  \n",
              "3  If your understanding of A.I. and Machine Lear...  \n",
              "4  Want to learn about applied Artificial Intelli...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9ffa51df-3f01-43ab-83f6-aab0b295f057\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>author</th>\n",
              "      <th>claps</th>\n",
              "      <th>reading_time</th>\n",
              "      <th>link</th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Justin Lee</td>\n",
              "      <td>8.3K</td>\n",
              "      <td>11</td>\n",
              "      <td>https://medium.com/swlh/chatbots-were-the-next...</td>\n",
              "      <td>Chatbots were the next big thing: what happene...</td>\n",
              "      <td>Oh, how the headlines blared:\\nChatbots were T...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Conor Dewey</td>\n",
              "      <td>1.4K</td>\n",
              "      <td>7</td>\n",
              "      <td>https://towardsdatascience.com/python-for-data...</td>\n",
              "      <td>Python for Data Science: 8 Concepts You May Ha...</td>\n",
              "      <td>If youâve ever found yourself looking up the...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>William Koehrsen</td>\n",
              "      <td>2.8K</td>\n",
              "      <td>11</td>\n",
              "      <td>https://towardsdatascience.com/automated-featu...</td>\n",
              "      <td>Automated Feature Engineering in Python â To...</td>\n",
              "      <td>Machine learning is increasingly moving from h...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Gant Laborde</td>\n",
              "      <td>1.3K</td>\n",
              "      <td>7</td>\n",
              "      <td>https://medium.freecodecamp.org/machine-learni...</td>\n",
              "      <td>Machine Learning: how to go from Zero to Hero ...</td>\n",
              "      <td>If your understanding of A.I. and Machine Lear...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Emmanuel Ameisen</td>\n",
              "      <td>935</td>\n",
              "      <td>11</td>\n",
              "      <td>https://blog.insightdatascience.com/reinforcem...</td>\n",
              "      <td>Reinforcement Learning from scratch â Insigh...</td>\n",
              "      <td>Want to learn about applied Artificial Intelli...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9ffa51df-3f01-43ab-83f6-aab0b295f057')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-9ffa51df-3f01-43ab-83f6-aab0b295f057 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-9ffa51df-3f01-43ab-83f6-aab0b295f057');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-718feaa7-f9bb-4f8b-9d9d-8d9c89e8b7d1\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-718feaa7-f9bb-4f8b-9d9d-8d9c89e8b7d1')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-718feaa7-f9bb-4f8b-9d9d-8d9c89e8b7d1 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 337,\n  \"fields\": [\n    {\n      \"column\": \"author\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 182,\n        \"samples\": [\n          \"Xu Wenhao\",\n          \"Illia Polosukhin\",\n          \"Bharath Raj\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"claps\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 163,\n        \"samples\": [\n          \"286\",\n          \"44K\",\n          \"558\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"reading_time\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 5,\n        \"min\": 2,\n        \"max\": 31,\n        \"num_unique_values\": 25,\n        \"samples\": [\n          13,\n          23,\n          11\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"link\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 337,\n        \"samples\": [\n          \"https://medium.com/deep-learning-101/algorithms-of-the-mind-10eb13f61fc4?source=tag_archive---------0----------------\",\n          \"https://towardsdatascience.com/using-deep-q-learning-in-fifa-18-to-perfect-the-art-of-free-kicks-f2e4e979ee66?source=---------7----------------\",\n          \"https://medium.com/@ageitgey/machine-learning-is-fun-part-4-modern-face-recognition-with-deep-learning-c3cffc121d78?source=tag_archive---------2----------------\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 230,\n        \"samples\": [\n          \"Write an AI to win at Pong from scratch with Reinforcement Learning\",\n          \"A simple deep learning model for stock price prediction using TensorFlow\",\n          \"What I learned from interviewing at multiple AI companies and start-ups\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 230,\n        \"samples\": [\n          \"There\\u00e2\\u0080\\u0099s a huge difference between reading about Reinforcement Learning and actually implementing it.\\nIn this post, you\\u00e2\\u0080\\u0099ll implement a Neural Network for Reinforcement Learning and see it learn more and more as it finally becomes good enough to beat the computer in Pong! You can play around with other such Atari games at the OpenAI Gym.\\nBy the end of this post, you\\u00e2\\u0080\\u0099ll be able to do the following:\\nThe code and the idea are all tightly based on Andrej Karpathy\\u00e2\\u0080\\u0099s blog post. The code in me_pong.py is intended to be a simpler to follow version of pong.py which was written by Dr. Karpathy.\\nTo follow along, you\\u00e2\\u0080\\u0099ll need to know the following:\\nIf you want a deeper dive into the material at hand, read the blog post on which all of this is based. This post is meant to be a simpler introduction to that material.\\nGreat! Let\\u00e2\\u0080\\u0099s get started.\\nWe are given the following:\\nCan we use these pieces to train our agent to beat the computer? Moreover, can we make our solution generic enough so it can be reused to win in games that aren\\u00e2\\u0080\\u0099t pong?\\nIndeed, we can! Andrej does this by building a Neural Network that takes in each image and outputs a command to our AI to move up or down.\\nWe can break this down a bit more into the following steps:\\nOur Neural Network, based heavily on Andrej\\u00e2\\u0080\\u0099s solution, will do the following:\\nOk now that we\\u00e2\\u0080\\u0099ve described the problem and its solution, let\\u00e2\\u0080\\u0099s get to writing some code!\\nWe\\u00e2\\u0080\\u0099re now going to follow the code in me_pong.py. Please keep it open and read along! The code starts here:\\nFirst, let\\u00e2\\u0080\\u0099s use OpenAI Gym to make a game environment and get our very first image of the game.\\nNext, we set a bunch of parameters based off of Andrej\\u00e2\\u0080\\u0099s blog post. We aren\\u00e2\\u0080\\u0099t going to worry about tuning them but note that you can probably get better performance by doing so. The parameters we will use are:\\nThen, we set counters, initial values, and the initial weights in our Neural Network.\\nWeights are stored in matrices. Layer 1 of our Neural Network is a 200 x 6400 matrix representing the weights for our hidden layer. For layer 1, element w1_ij represents the weight of neuron i for input pixel j in layer 1.\\nLayer 2 is a 200 x 1 matrix representing the weights of the output of the hidden layer on our final output. For layer 2, element w2_i represents the weights we place on the activation of neuron i in the hidden layer.\\nWe initialize each layer\\u00e2\\u0080\\u0099s weights with random numbers for now. We divide by the square root of the number of the dimension size to normalize our weights.\\nNext, we set up the initial parameters for RMSProp (a method for updating weights that we will discuss later). Don\\u00e2\\u0080\\u0099t worry too much about understanding what you see below. I\\u00e2\\u0080\\u0099m mainly bringing it up here so we can continue to follow along the main code block.\\nWe\\u00e2\\u0080\\u0099ll need to collect a bunch of observations and intermediate values across the episode and use those to compute the gradient at the end based on the result. The below sets up the arrays where we\\u00e2\\u0080\\u0099ll collect all that information.\\nOk we\\u00e2\\u0080\\u0099re all done with the setup! If you were following, it should look something like this:\\nPhew. Now for the fun part!\\nThe crux of our algorithm is going to live in a loop where we continually make a move and then learn based on the results of the move. We\\u00e2\\u0080\\u0099ll put everything in a while block for now but in reality you might set up a break condition to stop the process.\\nThe first step to our algorithm is processing the image of the game that OpenAI Gym passed us. We really don\\u00e2\\u0080\\u0099t care about the entire image - just certain details. We do this below:\\nLet\\u00e2\\u0080\\u0099s dive into preprocess_observations to see how we convert the image OpenAI Gym gives us into something we can use to train our Neural Network. The basic steps are:\\nNow that we\\u00e2\\u0080\\u0099ve preprocessed the observations, let\\u00e2\\u0080\\u0099s move on to actually sending the observations through our neural net to generate the probability of telling our AI to move up. Here are the steps we\\u00e2\\u0080\\u0099ll take:\\nHow exactly does apply_neural_nets take observations and weights and generate a probability of going up? This is just the forward pass of the Neural Network. Let\\u00e2\\u0080\\u0099s look at the code below for more information:\\nAs you can see, it\\u00e2\\u0080\\u0099s not many steps at all! Let\\u00e2\\u0080\\u0099s go step by step:\\nLet\\u00e2\\u0080\\u0099s return to the main algorithm and continue on. Now that we have obtained a probability of going up, we need to now record the results for later learning and choose an action to tell our AI to implement:\\nWe choose an action by flipping an imaginary coin that lands \\u00e2\\u0080\\u009cup\\u00e2\\u0080\\u009d with probability up_probability and down with 1 - up_probability. If it lands up, we choose tell our AI to go up and if not, we tell it to go down. We also\\nHaving done that, we pass the action to OpenAI Gym via env.step(action).\\nOk we\\u00e2\\u0080\\u0099ve covered the first half of the solution! We know what action to tell our AI to take. If you\\u00e2\\u0080\\u0099ve been following along, your code should look like this:\\nNow that we\\u00e2\\u0080\\u0099ve made our move, it\\u00e2\\u0080\\u0099s time to start learning so we figure out the right weights in our Neural Network!\\nLearning is all about seeing the result of the action (i.e. whether or not we won the round) and changing our weights accordingly. The first step to learning is asking the following question:\\nMathematically, this is just the derivative of our result with respect to the outputs of our final layer. If L is the value of our result to us and f is the function that gives us the activations of our final layer, this derivative is just \\u00e2\\u0088\\u0082L/\\u00e2\\u0088\\u0082f.\\nIn a binary classification context (i.e. we just have to tell the AI one of two actions, up or down), this derivative turns out to be\\nNote that \\u00cf\\u0083 in the above equation represents the sigmoid function. Read the Attribute Classification section here for more information about how we get the above derivative. We simplify this further below:\\nAfter one action(moving the paddle up or down), we don\\u00e2\\u0080\\u0099t really have an idea of whether or not this was the right action. So we\\u00e2\\u0080\\u0099re going to cheat and treat the action we end up sampling from our probability as the correct action.\\nOur predicion for this round is going to be the probability of going up we calculated. Using that, we have that \\u00e2\\u0088\\u0082L/\\u00e2\\u0088\\u0082f can be computed by\\nAwesome! We have the gradient per action.\\nThe next step is to figure out how we learn after the end of an episode (i.e. when we or our opponent miss the ball and someone gets a point). We do this by computing the policy gradient of the network at the end of each episode. The intuition here is that if we won the round, we\\u00e2\\u0080\\u0099d like our network to generate more of the actions that led to us winning. Alternatively, if we lose, we\\u00e2\\u0080\\u0099re going to try and generate less of these actions.\\nOpenAI Gym provides us the handy done variable to tell us when an episode finishes (i.e. we missed the ball or our opponent missed the ball). When we notice we are done, the first thing we do is compile all our observations and gradient calculations for the episode. This allows us to apply our learnings over all the actions in the episode.\\nNext, we want to learn in such a way that actions taken towards the end of an episode more heavily influence our learning than actions taken at the beginning. This is called discounting.\\nThink about it this way - if you moved up at the first frame of the episode, it probably had very little impact on whether or not you win. However, closer to the end of the episode, your actions probably have a much larger effect as they determine whether or not your paddle reaches the ball and how your paddle hits the ball.\\nWe\\u00e2\\u0080\\u0099re going to take this weighting into account by discounting our rewards such that rewards from earlier frames are discounted a lot more than rewards for later frames. After this, we\\u00e2\\u0080\\u0099re going to finally use backpropagation to compute the gradient (i.e. the direction we need to move our weights to improve).\\nLet\\u00e2\\u0080\\u0099s dig in a bit into how the policy gradient for the episode is computed. This is one of the most important parts of Reinforcement Learning as it\\u00e2\\u0080\\u0099s how our agent figures out how to improve over time.\\nTo begin with, if you haven\\u00e2\\u0080\\u0099t already, read this excerpt on backpropagation from Michael Nielsen\\u00e2\\u0080\\u0099s excellent free book on Deep Learning.\\nAs you\\u00e2\\u0080\\u0099ll see in that excerpt, there are four fundamental equations of backpropogation, a technique for computing the gradient for our weights.\\nOur goal is to find \\u00e2\\u0088\\u0082C/\\u00e2\\u0088\\u0082w1 (BP4), the derivative of the cost function with respect to the first layer\\u00e2\\u0080\\u0099s weights, and \\u00e2\\u0088\\u0082C/\\u00e2\\u0088\\u0082w2, the derivative of the cost function with respect to the second layer\\u00e2\\u0080\\u0099s weights. These gradients will help us understand what direction to move our weights in for the greatest improvement.\\nTo begin with, let\\u00e2\\u0080\\u0099s start with \\u00e2\\u0088\\u0082C/\\u00e2\\u0088\\u0082w2. If a^l2 is the activations of the hidden layer (layer 2), we see that the formula is:\\nIndeed, this is exactly what we do here:\\nNext, we need to calculate \\u00e2\\u0088\\u0082C/\\u00e2\\u0088\\u0082w1. The formula for that is:\\nand we also know that a^l1 is just our observation_values.\\nSo all we need now is \\u00ce\\u00b4^l2. Once we have that, we can calculate \\u00e2\\u0088\\u0082C/\\u00e2\\u0088\\u0082w1 and return. We do just that below:\\nIf you\\u00e2\\u0080\\u0099ve been following along, your function should look like this:\\nWith that, we\\u00e2\\u0080\\u0099ve finished backpropagation and computed our gradients!\\nAfter we have finished batch_size episodes, we finally update our weights for our Neural Network and implement our learnings.\\nTo update the weights, we simply apply RMSProp, an algorithm for updating weights described by Sebastian Reuder here.\\nWe implement this below:\\nThis is the step that tweaks our weights and allows us to get better over time.\\nThis is basically it! Putting it altogether it should look like this.\\nYou just coded a full Neural Network for playing Pong! Uncomment env.render() and run it for 3\\u00e2\\u0080\\u00934 days to see it finally beat the computer! You\\u00e2\\u0080\\u0099ll need to do some pickling as done in Andrej Karpathy\\u00e2\\u0080\\u0099s solution to be able to visualize your results when you win.\\nAccording to the blog post, this algorithm should take around 3 days of training on a Macbook to start beating the computer.\\nConsider tweaking the parameters or using Convolutional Neural Nets to boost the performance further.\\nIf you want a further primer into Neural Networks and Reinforcement Learning, there are some great resources to learn more (I work at Udacity as the Director of Machine Learning programs):\\nFrom a quick cheer to a standing ovation, clap to show how much you enjoyed this story.\\n@dhruvp. VP Eng @Athelas. MIT Math and CS Undergrad \\u00e2\\u0080\\u009913. MIT CS Masters \\u00e2\\u0080\\u009914. Previously: Director of AI Programs @ Udacity.\\n\",\n          \"For a recent hackathon that we did at STATWORX, some of our team members scraped minutely S&P 500 data from the Google Finance API. The data consisted of index as well as stock prices of the S&P\\u00e2\\u0080\\u0099s 500 constituents. Having this data at hand, the idea of developing a deep learning model for predicting the S&P 500 index based on the 500 constituents prices one minute ago came immediately on my mind.\\nPlaying around with the data and building the deep learning model with TensorFlow was fun and so I decided to write my first Medium.com story: a little TensorFlow tutorial on predicting S&P 500 stock prices. What you will read is not an in-depth tutorial, but more a high-level introduction to the important building blocks and concepts of TensorFlow models. The Python code I\\u00e2\\u0080\\u0099ve created is not optimized for efficiency but understandability. The dataset I\\u00e2\\u0080\\u0099ve used can be downloaded from here (40MB).\\nOur team exported the scraped stock data from our scraping server as a csv file. The dataset contains n = 41266 minutes of data ranging from April to August 2017 on 500 stocks as well as the total S&P 500 index price. Index and stocks are arranged in wide format.\\nThe data was already cleaned and prepared, meaning missing stock and index prices were LOCF\\u00e2\\u0080\\u0099ed (last observation carried forward), so that the file did not contain any missing values.\\nA quick look at the S&P time series using pyplot.plot(data['SP500']):\\nNote: This is actually the lead of the S&P 500 index, meaning, its value is shifted 1 minute into the future. This operation is necessary since we want to predict the next minute of the index and not the current minute.\\nThe dataset was split into training and test data. The training data contained 80% of the total dataset. The data was not shuffled but sequentially sliced. The training data ranges from April to approx. end of July 2017, the test data ends end of August 2017.\\nThere are a lot of different approaches to time series cross validation, such as rolling forecasts with and without refitting or more elaborate concepts such as time series bootstrap resampling. The latter involves repeated samples from the remainder of the seasonal decomposition of the time series in order to simulate samples that follow the same seasonal pattern as the original time series but are not exact copies of its values.\\nMost neural network architectures benefit from scaling the inputs (sometimes also the output). Why? Because most common activation functions of the network\\u00e2\\u0080\\u0099s neurons such as tanh or sigmoid are defined on the [-1, 1] or [0, 1] interval respectively. Nowadays, rectified linear unit (ReLU) activations are commonly used activations which are unbounded on the axis of possible activation values. However, we will scale both the inputs and targets anyway. Scaling can be easily accomplished in Python using sklearn\\u00e2\\u0080\\u0099s MinMaxScaler.\\nRemark: Caution must be undertaken regarding what part of the data is scaled and when. A common mistake is to scale the whole dataset before training and test split are being applied. Why is this a mistake? Because scaling invokes the calculation of statistics e.g. the min/max of a variable. When performing time series forecasting in real life, you do not have information from future observations at the time of forecasting. Therefore, calculation of scaling statistics has to be conducted on training data and must then be applied to the test data. Otherwise, you use future information at the time of forecasting which commonly biases forecasting metrics in a positive direction.\\nTensorFlow is a great piece of software and currently the leading deep learning and neural network computation framework. It is based on a C++ low level backend but is usually controlled via Python (there is also a neat TensorFlow library for R, maintained by RStudio). TensorFlow operates on a graph representation of the underlying computational task. This approach allows the user to specify mathematical operations as elements in a graph of data, variables and operators. Since neural networks are actually graphs of data and mathematical operations, TensorFlow is just perfect for neural networks and deep learning. Check out this simple example (stolen from our deep learning introduction from our blog):\\nIn the figure above, two numbers are supposed to be added. Those numbers are stored in two variables, a and b. The two values are flowing through the graph and arrive at the square node, where they are being added. The result of the addition is stored into another variable, c. Actually, a, b and c can be considered as placeholders. Any numbers that are fed into a and b get added and are stored into c. This is exactly how TensorFlow works. The user defines an abstract representation of the model (neural network) through placeholders and variables. Afterwards, the placeholders get \\\"filled\\\" with real data and the actual computations take place. The following code implements the toy example from above in TensorFlow:\\nAfter having imported the TensorFlow library, two placeholders are defined using tf.placeholder(). They correspond to the two blue circles on the left of the image above. Afterwards, the mathematical addition is defined via tf.add(). The result of the computation is c = 9. With placeholders set up, the graph can be executed with any integer value for a and b. Of course, the former problem is just a toy example. The required graphs and computations in a neural network are much more complex.\\nAs mentioned before, it all starts with placeholders. We need two placeholders in order to fit our model: X contains the network's inputs (the stock prices of all S&P 500 constituents at time T = t) and Y the network's outputs (the index value of the S&P 500 at time T = t + 1).\\nThe shape of the placeholders correspond to [None, n_stocks] with [None] meaning that the inputs are a 2-dimensional matrix and the outputs are a 1-dimensional vector. It is crucial to understand which input and output dimensions the neural net needs in order to design it properly.\\nThe None argument indicates that at this point we do not yet know the number of observations that flow through the neural net graph in each batch, so we keep if flexible. We will later define the variable batch_size that controls the number of observations per training batch.\\nBesides placeholders, variables are another cornerstone of the TensorFlow universe. While placeholders are used to store input and target data in the graph, variables are used as flexible containers within the graph that are allowed to change during graph execution. Weights and biases are represented as variables in order to adapt during training. Variables need to be initialized, prior to model training. We will get into that a litte later in more detail.\\nThe model consists of four hidden layers. The first layer contains 1024 neurons, slightly more than double the size of the inputs. Subsequent hidden layers are always half the size of the previous layer, which means 512, 256 and finally 128 neurons. A reduction of the number of neurons for each subsequent layer compresses the information the network identifies in the previous layers. Of course, other network architectures and neuron configurations are possible but are out of scope for this introduction level article.\\nIt is important to understand the required variable dimensions between input, hidden and output layers. As a rule of thumb in multilayer perceptrons (MLPs, the type of networks used here), the second dimension of the previous layer is the first dimension in the current layer for weight matrices. This might sound complicated but is essentially just each layer passing its output as input to the next layer. The biases dimension equals the second dimension of the current layer\\u00e2\\u0080\\u0099s weight matrix, which corresponds the number of neurons in this layer.\\nAfter definition of the required weight and bias variables, the network topology, the architecture of the network, needs to be specified. Hereby, placeholders (data) and variables (weighs and biases) need to be combined into a system of sequential matrix multiplications.\\nFurthermore, the hidden layers of the network are transformed by activation functions. Activation functions are important elements of the network architecture since they introduce non-linearity to the system. There are dozens of possible activation functions out there, one of the most common is the rectified linear unit (ReLU) which will also be used in this model.\\nThe image below illustrates the network architecture. The model consists of three major building blocks. The input layer, the hidden layers and the output layer. This architecture is called a feedforward network. Feedforward indicates that the batch of data solely flows from left to right. Other network architectures, such as recurrent neural networks, also allow data flowing \\u00e2\\u0080\\u009cbackwards\\u00e2\\u0080\\u009d in the network.\\nThe cost function of the network is used to generate a measure of deviation between the network\\u00e2\\u0080\\u0099s predictions and the actual observed training targets. For regression problems, the mean squared error (MSE) function is commonly used. MSE computes the average squared deviation between predictions and targets. Basically, any differentiable function can be implemented in order to compute a deviation measure between predictions and targets.\\nHowever, the MSE exhibits certain properties that are advantageous for the general optimization problem to be solved.\\nThe optimizer takes care of the necessary computations that are used to adapt the network\\u00e2\\u0080\\u0099s weight and bias variables during training. Those computations invoke the calculation of so called gradients, that indicate the direction in which the weights and biases have to be changed during training in order to minimize the network\\u00e2\\u0080\\u0099s cost function. The development of stable and speedy optimizers is a major field in neural network an deep learning research.\\nHere the Adam Optimizer is used, which is one of the current default optimizers in deep learning development. Adam stands for \\u00e2\\u0080\\u009cAdaptive Moment Estimation\\u00e2\\u0080\\u009d and can be considered as a combination between two other popular optimizers AdaGrad and RMSProp.\\nInitializers are used to initialize the network\\u00e2\\u0080\\u0099s variables before training. Since neural networks are trained using numerical optimization techniques, the starting point of the optimization problem is one the key factors to find good solutions to the underlying problem. There are different initializers available in TensorFlow, each with different initialization approaches. Here, I use the tf.variance_scaling_initializer(), which is one of the default initialization strategies.\\nNote, that with TensorFlow it is possible to define multiple initialization functions for different variables within the graph. However, in most cases, a unified initialization is sufficient.\\nAfter having defined the placeholders, variables, initializers, cost functions and optimizers of the network, the model needs to be trained. Usually, this is done by minibatch training. During minibatch training random data samples of n = batch_size are drawn from the training data and fed into the network. The training dataset gets divided into n / batch_size batches that are sequentially fed into the network. At this point the placeholders X and Y come into play. They store the input and target data and present them to the network as inputs and targets.\\nA sampled data batch of X flows through the network until it reaches the output layer. There, TensorFlow compares the models predictions against the actual observed targets Y in the current batch. Afterwards, TensorFlow conducts an optimization step and updates the networks parameters, corresponding to the selected learning scheme. After having updated the weights and biases, the next batch is sampled and the process repeats itself. The procedure continues until all batches have been presented to the network. One full sweep over all batches is called an epoch.\\nThe training of the network stops once the maximum number of epochs is reached or another stopping criterion defined by the user applies.\\nDuring the training, we evaluate the networks predictions on the test set \\u00e2\\u0080\\u0094 the data which is not learned, but set aside \\u00e2\\u0080\\u0094 for every 5th batch and visualize it. Additionally, the images are exported to disk and later combined into a video animation of the training process (see below). The model quickly learns the shape und location of the time series in the test data and is able to produce an accurate prediction after some epochs. Nice!\\nOne can see that the networks rapidly adapts to the basic shape of the time series and continues to learn finer patterns of the data. This also corresponds to the Adam learning scheme that lowers the learning rate during model training in order not to overshoot the optimization minimum. After 10 epochs, we have a pretty close fit to the test data! The final test MSE equals 0.00078 (it is very low, because the target is scaled). The mean absolute percentage error of the forecast on the test set is equal to 5.31% which is pretty good. Note, that this is just a fit to the test data, no actual out of sample metrics in a real world scenario.\\nPlease note that there are tons of ways of further improving this result: design of layers and neurons, choosing different initialization and activation schemes, introduction of dropout layers of neurons, early stopping and so on. Furthermore, different types of deep learning models, such as recurrent neural networks might achieve better performance on this task. However, this is not the scope of this introductory post.\\nThe release of TensorFlow was a landmark event in deep learning research. Its flexibility and performance allows researchers to develop all kinds of sophisticated neural network architectures as well as other ML algorithms. However, flexibility comes at the cost of longer time-to-model cycles compared to higher level APIs such as Keras or MxNet. Nonetheless, I am sure that TensorFlow will make its way to the de-facto standard in neural network and deep learning development in research and practical applications. Many of our customers are already using TensorFlow or start developing projects that employ TensorFlow models. Also our data science consultants at STATWORX are heavily using TensorFlow for deep learning and neural net research and development. Let\\u00e2\\u0080\\u0099s see what Google has planned for the future of TensorFlow. One thing that is missing, at least in my opinion, is a neat graphical user interface for designing and developing neural net architectures with TensorFlow backend. Maybe, this is something Google is already working on ;)\\nIf you have any comments or questions on my first Medium story, feel free to comment below! I will try to answer them. Also, feel free to use my code or share this story with your peers on social platforms of your choice.\\nUpdate: I\\u00e2\\u0080\\u0099ve added both the Python script as well as a (zipped) dataset to a Github repository. Feel free to clone and fork.\\nLastly, follow me on: Twitter | LinkedIn\\nFrom a quick cheer to a standing ovation, clap to show how much you enjoyed this story.\\nCEO @ STATWORX. Doing data science, stats and ML for over a decade. Food, wine and cocktail enthusiast. Check our website: https://www.statworx.com\\nHighlights from Machine Learning Research, Projects and Learning Materials. From and For ML Scientists, Engineers an Enthusiasts.\\n\",\n          \"Over the past 8 months, I\\u00e2\\u0080\\u0099ve been interviewing at various companies like Google\\u00e2\\u0080\\u0099s DeepMind, Wadhwani Institute of AI, Microsoft, Ola, Fractal Analytics, and a few others primarily for the roles \\u00e2\\u0080\\u0094 Data Scientist, Software Engineer & Research Engineer. In the process, not only did I get an opportunity to interact with many great minds, but also had a peek at myself along with a sense of what people really look for when interviewing someone. I believe that if I\\u00e2\\u0080\\u0099d had this knowledge before, I could have avoided many mistakes and have prepared in a much better manner, which is what the motivation behind this post is, to be able to help someone bag their dream place of work.\\nThis post arose from a discussion with one of my juniors on the lack of really fulfilling job opportunities offered through campus placements for people working in AI. Also, when I was preparing, I noticed people using a lot of resources but as per my experience over the past months, I realised that one can do away with a few minimal ones for most roles in AI, all of which I\\u00e2\\u0080\\u0099m going to mention at the end of the post. I begin with How to get noticed a.k.a. the interview. Then I provide a List of companies and start-ups to apply, which is followed by How to ace that interview. Based on whatever experience I\\u00e2\\u0080\\u0099ve had, I add a section on What we should strive to work for. I conclude with Minimal Resources you need for preparation.\\nNOTE: For people who are sitting for campus placements, there are two things I\\u00e2\\u0080\\u0099d like to add. Firstly, most of what I\\u00e2\\u0080\\u0099m going to say (except for the last one maybe) is not going to be relevant to you for placements. But, and this is my second point, as I mentioned before, opportunities on campus are mostly in software engineering roles having no intersection with AI. So, this post is specifically meant for people who want to work on solving interesting problems using AI. Also, I want to add that I haven\\u00e2\\u0080\\u0099t cleared all of these interviews but I guess that\\u00e2\\u0080\\u0099s the essence of failure \\u00e2\\u0080\\u0094 it\\u00e2\\u0080\\u0099s the greatest teacher! The things that I mention here may not all be useful but these are things that I did and there\\u00e2\\u0080\\u0099s no way for me to know what might have ended up making my case stronger.\\nTo be honest, this step is the most important one. What makes off-campus placements so tough and exhausting is getting the recruiter to actually go through your profile among the plethora of applications that they get. Having a contact inside the organisation place a referral for you would make it quite easy, but, in general, this part can be sub-divided into three keys steps:\\na) Do the regulatory preparation and do that well: So, with regulatory preparation, I mean \\u00e2\\u0080\\u0094a LinkedIn profile, a Github profile, a portfolio website and a well-polished CV. Firstly, your CV should be really neat and concise. Follow this guide by Udacity for cleaning up your CV \\u00e2\\u0080\\u0094 Resume Revamp. It has everything that I intend to say and I\\u00e2\\u0080\\u0099ve been using it as a reference guide myself. As for the CV template, some of the in-built formats on Overleaf are quite nice. I personally use deedy-resume. Here\\u00e2\\u0080\\u0099s a preview:\\nAs it can be seen, a lot of content can be fit into one page. However, if you really do need more than that, then the format linked above would not work directly. Instead, you can find a modified multi-page format of the same here. The next most important thing to mention is your Github profile. A lot of people underestimate the potential of this, just because unlike LinkedIn, it doesn\\u00e2\\u0080\\u0099t have a \\u00e2\\u0080\\u009cWho Viewed Your Profile\\u00e2\\u0080\\u009d option. People DO go through your Github because that\\u00e2\\u0080\\u0099s the only way they have to validate what you have mentioned in your CV, given that there\\u00e2\\u0080\\u0099s a lot of noise today with people associating all kinds of buzzwords with their profile. Especially for data science, open-source has a big role to play too with majority of the tools, implementations of various algorithms, lists of learning resources, all being open-sourced. I discuss the benefits of getting involved in Open-Source and how one can start from scratch in an earlier post here. The bare minimum for now should be:\\n\\u00e2\\u0080\\u00a2 Create a Github account if you don\\u00e2\\u0080\\u0099t already have one.\\u00e2\\u0080\\u00a2 Create a repository for each of the projects that you have done.\\u00e2\\u0080\\u00a2 Add documentation with clear instructions on how to run the code\\u00e2\\u0080\\u00a2 Add documentation for each file mentioning the role of each function, the meaning of each parameter, proper formatting (e.g. PEP8 for Python) along with a script to automate the previous step (Optional).\\nMoving on, the third step is what most people lack, which is having a portfolio website demonstrating their experience and personal projects. Making a portfolio indicates that you are really serious about getting into the field and adds a lot of points to the authenticity factor. Also, you generally have space constraints on your CV and tend to miss out on a lot of details. You can use your portfolio to really delve deep into the details if you want to and it\\u00e2\\u0080\\u0099s highly recommended to include some sort of visualisation or demonstration of the project/idea. It\\u00e2\\u0080\\u0099s really easy to create one too as there are a lot of free platforms with drag-and-drop features making the process really painless. I personally use Weebly which is a widely used tool. It\\u00e2\\u0080\\u0099s better to have a reference to begin with. There are a lot of awesome ones out there but I referred to Deshraj Yadav\\u00e2\\u0080\\u0099s personal website to begin with making mine:\\nFinally, a lot of recruiters and start-ups have nowadays started using LinkedIn as their go-to platform for hiring. A lot of good jobs get posted there. Apart from recruiters, the people working at influential positions are quite active there as well. So, if you can grab their attention, you have a good chance of getting in too. Apart from that, maintaining a clean profile is necessary for people to have the will to connect with you. An important part of LinkedIn is their search tool and for you to show up, you must have the relevant keywords interspersed over your profile. It took me a lot of iterations and re-evaluations to finally have a decent one. Also, you should definitely ask people with or under whom you\\u00e2\\u0080\\u0099ve worked with to endorse you for your skills and add a recommendation talking about their experience of working with you. All of this increases your chance of actually getting noticed. I\\u00e2\\u0080\\u0099ll again point towards Udacity\\u00e2\\u0080\\u0099s guide for LinkedIn and Github profiles.\\nAll this might seem like a lot, but remember that you don\\u00e2\\u0080\\u0099t need to do it in a single day or even a week or a month. It\\u00e2\\u0080\\u0099s a process, it never ends. Setting up everything at first would definitely take some effort but once it\\u00e2\\u0080\\u0099s there and you keep updating it regularly as events around you keep happening, you\\u00e2\\u0080\\u0099ll not only find it to be quite easy, but also you\\u00e2\\u0080\\u0099ll be able to talk about yourself anywhere anytime without having to explicitly prepare for it because you become so aware about yourself.\\nb) Stay authentic: I\\u00e2\\u0080\\u0099ve seen a lot of people do this mistake of presenting themselves as per different job profiles. According to me, it\\u00e2\\u0080\\u0099s always better to first decide what actually interests you, what would you be happy doing and then search for relevant opportunities; not the other way round. The fact that the demand for AI talent surpasses the supply for the same gives you this opportunity. Spending time on your regulatory preparation mentioned above would give you an all-around perspective on yourself and help make this decision easier. Also, you won\\u00e2\\u0080\\u0099t need to prepare answers to various kinds of questions that you get asked during an interview. Most of them would come out naturally as you\\u00e2\\u0080\\u0099d be talking about something you really care about.\\nc) Networking: Once you\\u00e2\\u0080\\u0099re done with a), figured out b), Networking is what will actually help you get there. If you don\\u00e2\\u0080\\u0099t talk to people, you miss out on hearing about many opportunities that you might have a good shot at. It\\u00e2\\u0080\\u0099s important to keep connecting with new people each day, if not physically, then on LinkedIn, so that upon compounding it after many days, you have a large and strong network. Networking is NOT messaging people to place a referral for you. When I was starting off, I did this mistake way too often until I stumbled upon this excellent article by Mark Meloon, where he talks about the importance of building a real connection with people by offering our help first. Another important step in networking is to get your content out. For example, if you\\u00e2\\u0080\\u0099re good at something, blog about it and share that blog on Facebook and LinkedIn. Not only does this help others, it helps you as well. Once you have a good enough network, your visibility increases multi-fold. You never know how one person from your network liking or commenting on your posts, may help you reach out to a much broader audience including people who might be looking for someone of your expertise.\\nI\\u00e2\\u0080\\u0099m presenting this list in alphabetical order to avoid the misinterpretation of any specific preference. However, I do place a \\u00e2\\u0080\\u009c*\\u00e2\\u0080\\u009d on the ones that I\\u00e2\\u0080\\u0099d personally recommend. This recommendation is based on either of the following: mission statement, people, personal interaction or scope of learning. More than 1 \\u00e2\\u0080\\u009c*\\u00e2\\u0080\\u009d is purely based on the 2nd and 3rd factors.\\nYour interview begins the moment you have entered the room and a lot of things can happen between that moment and the time when you\\u00e2\\u0080\\u0099re asked to introduce yourself \\u00e2\\u0080\\u0094 your body language and the fact that you\\u00e2\\u0080\\u0099re smiling while greeting them plays a big role, especially when you\\u00e2\\u0080\\u0099re interviewing for a start-up as culture-fit is something that they extremely care about. You need to understand that as much as the interviewer is a stranger to you, you\\u00e2\\u0080\\u0099re a stranger to him/her too. So, they\\u00e2\\u0080\\u0099re probably just as nervous as you are.\\nIt\\u00e2\\u0080\\u0099s important to view the interview as more of a conversation between yourself and the interviewer. Both of you are looking for a mutual fit \\u00e2\\u0080\\u0094 you are looking for an awesome place to work at and the interviewer is looking for an awesome person (like you) to work with. So, make sure that you\\u00e2\\u0080\\u0099re feeling good about yourself and that you take the charge of making the initial moments of your conversation pleasant for them. And the easiest way I know how to make that happen is to smile.\\nThere are mostly two types of interviews \\u00e2\\u0080\\u0094 one, where the interviewer has come with come prepared set of questions and is going to just ask you just that irrespective of your profile and the second, where the interview is based on your CV. I\\u00e2\\u0080\\u0099ll start with the second one.\\nThis kind of interview generally begins with a \\u00e2\\u0080\\u009cCan you tell me a bit about yourself?\\u00e2\\u0080\\u009d. At this point, 2 things are a big NO \\u00e2\\u0080\\u0094 talking about your GPA in college and talking about your projects in detail. An ideal statement should be about a minute or two long, should give a good idea on what have you been doing till now, and it\\u00e2\\u0080\\u0099s not restricted to academics. You can talk about your hobbies like reading books, playing sports, meditation, etc \\u00e2\\u0080\\u0094 basically, anything that contributes to defining you. The interviewer will then take something that you talk about here as a cue for his next question, and then the technical part of the interview begins. The motive of this kind of interview is to really check whether whatever you have written on your CV is true or not:\\nThere would be a lot of questions on what could be done differently or if \\u00e2\\u0080\\u009cX\\u00e2\\u0080\\u009d was used instead of \\u00e2\\u0080\\u009cY\\u00e2\\u0080\\u009d, what would have happened. At this point, it\\u00e2\\u0080\\u0099s important to know the kind of trade-offs that is usually made during implementation, for e.g. if the interviewer says that using a more complex model would have given better results, then you might say that you actually had less data to work with and that would have lead to overfitting. In one of the interviews, I was given a case-study to work on and it involved designing algorithms for a real-world use case. I\\u00e2\\u0080\\u0099ve noticed that once I\\u00e2\\u0080\\u0099ve been given the green flag to talk about a project, the interviewers really like it when I talk about it in the following flow:\\nProblem > 1 or 2 previous approaches > Our approach > Result > Intuition\\nThe other kind of interview is really just to test your basic knowledge. Don\\u00e2\\u0080\\u0099t expect those questions to be too hard. But they would definitely scratch every bit of the basics that you should be having, mainly based around Linear Algebra, Probability, Statistics, Optimisation, Machine Learning and/or Deep Learning. The resources mentioned in the Minimal Resources you need for preparation section should suffice, but make sure that you don\\u00e2\\u0080\\u0099t miss out one bit among them. The catch here is the amount of time you take to answer those questions. Since these cover the basics, they expect that you should be answering them almost instantly. So, do your preparation accordingly.\\nThroughout the process, it\\u00e2\\u0080\\u0099s important to be confident and honest about what you know and what you don\\u00e2\\u0080\\u0099t know. If there\\u00e2\\u0080\\u0099s a question that you\\u00e2\\u0080\\u0099re certain you have no idea about, say it upfront rather than making \\u00e2\\u0080\\u009cAah\\u00e2\\u0080\\u009d, \\u00e2\\u0080\\u009cUm\\u00e2\\u0080\\u009d sounds. If some concept is really important but you are struggling with answering it, the interviewer would generally (depending on how you did in the initial parts) be happy to give you a hint or guide you towards the right solution. It\\u00e2\\u0080\\u0099s a big plus if you manage to pick their hints and arrive at the correct solution. Try to not get nervous and the best way to avoid that is by, again, smiling.\\nNow we come to the conclusion of the interview where the interviewer would ask you if you have any questions for them. It\\u00e2\\u0080\\u0099s really easy to think that your interview is done and just say that you have nothing to ask. I know many people who got rejected just because of failing at this last question. As I mentioned before, it\\u00e2\\u0080\\u0099s not only you who is being interviewed. You are also looking for a mutual fit with the company itself. So, it\\u00e2\\u0080\\u0099s quite obvious that if you really want to join a place, you must have many questions regarding the work culture there or what kind of role are they seeing you in. It can be as simple as being curious about the person interviewing you. There\\u00e2\\u0080\\u0099s always something to learn from everything around you and you should make sure that you leave the interviewer with the impression that you\\u00e2\\u0080\\u0099re truly interested in being a part of their team. A final question that I\\u00e2\\u0080\\u0099ve started asking all my interviewers, is for a feedback on what they might want me to improve on. This has helped me tremendously and I still remember every feedback that I\\u00e2\\u0080\\u0099ve gotten which I\\u00e2\\u0080\\u0099ve incorporated into my daily life.\\nThat\\u00e2\\u0080\\u0099s it. Based on my experience, if you\\u00e2\\u0080\\u0099re just honest about yourself, are competent, truly care about the company you\\u00e2\\u0080\\u0099re interviewing for and have the right mindset, you should have ticked all the right boxes and should be getting a congratulatory mail soon \\u00f0\\u009f\\u0098\\u0084\\nWe live in an era full of opportunities and that applies to anything that you love. You just need to strive to become the best at it and you will find a way to monetise it. As Gary Vaynerchuk (just follow him already) says:\\nThis is a great time to be working in AI and if you\\u00e2\\u0080\\u0099re truly passionate about it, you have so much that you can do with AI. You can empower so many people that have always been under-represented. We keep nagging about the problems surrounding us, but there\\u00e2\\u0080\\u0099s been never such a time where common people like us can actually do something about those problems, rather than just complaining. Jeffrey Hammerbacher (Founder, Cloudera) had famously said:\\nWe can do so much with AI than we can ever imagine. There are many extremely challenging problems out there which require incredibly smart people like you to put your head down on and solve. You can make many lives better. Time to let go of what is \\u00e2\\u0080\\u009ccool\\u00e2\\u0080\\u009d, or what would \\u00e2\\u0080\\u009clook good\\u00e2\\u0080\\u009d. THINK and CHOOSE wisely.\\nAny Data Science interview comprises of questions mostly of a subset of the following four categories: Computer Science, Math, Statistics and Machine Learning.\\nIf you\\u00e2\\u0080\\u0099re not familiar with the math behind Deep Learning, then you should consider going over my last post for resources to understand them. However, if you are comfortable, I\\u00e2\\u0080\\u0099ve found that the chapters 2, 3 and 4 of the Deep Learning Book are enough to prepare/revise for theoretical questions during such interviews. I\\u00e2\\u0080\\u0099ve been preparing summaries for a few chapters which you can refer to where I\\u00e2\\u0080\\u0099ve tried to even explain a few concepts that I found challenging to understand at first, in case you are not willing to go through the entire chapters. And if you\\u00e2\\u0080\\u0099ve already done a course on probability, you should be comfortable answering a few numerical as well. For stats, covering these topics should be enough.\\nNow, the range of questions here can vary depending on the type of position you are applying for. If it\\u00e2\\u0080\\u0099s a more traditional Machine Learning based interview where they want to check your basic knowledge in ML, you can complete any one of the following courses:- Machine Learning by Andrew Ng \\u00e2\\u0080\\u0094 CS 229- Machine Learning course by Caltech Professor Yaser Abu-Mostafa\\nImportant topics are: Supervised Learning (Classification, Regression, SVM, Decision Tree, Random Forests, Logistic Regression, Multi-layer Perceptron, Parameter Estimation, Bayes\\u00e2\\u0080\\u0099 Decision Rule), Unsupervised Learning (K-means Clustering, Gaussian Mixture Models), Dimensionality Reduction (PCA).\\nNow, if you\\u00e2\\u0080\\u0099re applying for a more advanced position, there\\u00e2\\u0080\\u0099s a high chance that you might be questioned on Deep Learning. In that case, you should be very comfortable with Convolutional Neural Networks (CNNs) and/or (depending upon what you\\u00e2\\u0080\\u0099ve worked on) Recurrent Neural Networks (RNNs) and their variants. And by being comfortable, you must know what is the fundamental idea behind Deep Learning, how CNNs/RNNs actually worked, what kind of architectures have been proposed and what has been the motivation behind those architectural changes. Now, there\\u00e2\\u0080\\u0099s no shortcut for this. Either you understand them or you put enough time to understand them. For CNNs, the recommended resource is Stanford\\u00e2\\u0080\\u0099s CS 231N and CS 224N for RNNs. I found this Neural Network class by Hugo Larochelle to be really enlightening too. Refer this for a quick refresher too. Udacity coming to the aid here too. By now, you should have figured out that Udacity is a really important place for an ML practitioner. There are not a lot of places working on Reinforcement Learning (RL) in India and I too am not experienced in RL as of now. So, that\\u00e2\\u0080\\u0099s one thing to add to this post sometime in the future.\\nGetting placed off-campus is a long journey of self-realisation. I realise that this has been another long post and I\\u00e2\\u0080\\u0099m again extremely grateful to you for valuing my thoughts. I hope that this post finds a way of being useful to you and that it helped you in some way to prepare for your next Data Science interview better. If it did, I request you to really think about what I talk about in What we should strive to work for.\\nI\\u00e2\\u0080\\u0099m very thankful to my friends from IIT Guwahati for their helpful feedback, especially Ameya Godbole, Kothapalli Vignesh and Prabal Jain. A majority of what I mention here, like \\u00e2\\u0080\\u009cviewing an interview as a conversation\\u00e2\\u0080\\u009d and \\u00e2\\u0080\\u009cseeking feedback from our interviewers\\u00e2\\u0080\\u009d, arose from multiple discussions with Prabal who has been advising me constantly on how I can improve my interviewing skills.\\nThis story is published in Noteworthy, where thousands come every day to learn about the people & ideas shaping the products we love.\\nFollow our publication to see more product & design stories featured by the Journal team.\\nFrom a quick cheer to a standing ovation, clap to show how much you enjoyed this story.\\nAI Fanatic \\u00e2\\u0080\\u00a2 Math Lover \\u00e2\\u0080\\u00a2 Dreamer\\nThe official Journal blog\\n\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 46
        }
      ],
      "source": [
        "df = pd.read_csv(\"articles.csv\",encoding=\"unicode_escape\")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "ca2202a7",
      "metadata": {
        "id": "ca2202a7",
        "outputId": "50fe22c4-7249-48cc-9d4f-ec3fc0db4254",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Oh, how the headlines blared:\\nChatbots were The Next Big Thing.\\nOur hopes were sky high. Bright-eyed and bushy-tailed, the industry was ripe for a new era of innovation: it was time to start socializing with machines.\\nAnd why wouldnâ\\x80\\x99t they be? All the road signs pointed towards insane success.\\nAt the Mobile World Congress 2017, chatbots were the main headliners. The conference organizers cited an â\\x80\\x98overwhelming acceptance at the event of the inevitable shift of focus for brands and corporates to chatbotsâ\\x80\\x99.\\nIn fact, the only significant question around chatbots was who would monopolize the field, not whether chatbots would take off in the first place:\\nOne year on, we have an answer to that question.\\nNo.\\nBecause there isnâ\\x80\\x99t even an ecosystem for a platform to dominate.\\nChatbots werenâ\\x80\\x99t the first technological development to be talked up in grandiose terms and then slump spectacularly.\\nThe age-old hype cycle unfolded in familiar fashion...\\nExpectations built, built, and then..... It all kind of fizzled out.\\nThe predicted paradim shift didnâ\\x80\\x99t materialize.\\nAnd apps are, tellingly, still alive and well.\\nWe look back at our breathless optimism and turn to each other, slightly baffled:\\nâ\\x80\\x9cis that it? THAT was the chatbot revolution we were promised?â\\x80\\x9d\\nDigitâ\\x80\\x99s Ethan Bloch sums up the general consensus:\\nAccording to Dave Feldman, Vice President of Product Design at Heap, chatbots didnâ\\x80\\x99t just take on one difficult problem and fail: they took on several and failed all of them.\\nBots can interface with users in different ways. The big divide is text vs. speech. In the beginning (of computer interfaces) was the (written) word.\\nUsers had to type commands manually into a machine to get anything done.\\nThen, graphical user interfaces (GUIs) came along and saved the day. We became entranced by windows, mouse clicks, icons. And hey, we eventually got color, too!\\nMeanwhile, a bunch of research scientists were busily developing natural language (NL) interfaces to databases, instead of having to learn an arcane database query language.\\nAnother bunch of scientists were developing speech-processing software so that you could just speak to your computer, rather than having to type. This turned out to be a whole lot more difficult than anyone originally realised:\\nThe next item on the agenda was holding a two-way dialog with a machine. Hereâ\\x80\\x99s an example dialog (dating back to the 1990s) with VCR setup system:\\nPretty cool, right? The system takes turns in collaborative way, and does a smart job of figuring out what the user wants.\\nIt was carefully crafted to deal with conversations involving VCRs, and could only operate within strict limitations.\\nModern day bots, whether they use typed or spoken input, have to face all these challenges, but also work in an efficient and scalable way on a variety of platforms.\\nBasically, weâ\\x80\\x99re still trying to achieve the same innovations we were 30 years ago.\\nHereâ\\x80\\x99s where I think weâ\\x80\\x99re going wrong:\\nAn oversized assumption has been that apps are â\\x80\\x98overâ\\x80\\x99, and would be replaced by bots.\\nBy pitting two such disparate concepts against one another (instead of seeing them as separate entities designed to serve different purposes) we discouraged bot development.\\nYou might remember a similar war cry when apps first came onto the scene ten years ago: but do you remember when apps replaced the internet?\\nItâ\\x80\\x99s said that a new product or service needs to be two of the following: better, cheaper, or faster. Are chatbots cheaper or faster than apps? No â\\x80\\x94 not yet, at least.\\nWhether theyâ\\x80\\x99re â\\x80\\x98betterâ\\x80\\x99 is subjective, but I think itâ\\x80\\x99s fair to say that todayâ\\x80\\x99s best bot isnâ\\x80\\x99t comparable to todayâ\\x80\\x99s best app.\\nPlus, nobody thinks that using Lyft is too complicated, or that itâ\\x80\\x99s too hard to order food or buy a dress on an app. What is too complicated is trying to complete these tasks with a bot â\\x80\\x94 and having the bot fail.\\nA great bot can be about as useful as an average app. When it comes to rich, sophisticated, multi-layered apps, thereâ\\x80\\x99s no competition.\\nThatâ\\x80\\x99s because machines let us access vast and complex information systems, and the early graphical information systems were a revolutionary leap forward in helping us locate those systems.\\nModern-day apps benefit from decades of research and experimentation. Why would we throw this away?\\nBut, if we swap the word â\\x80\\x98replaceâ\\x80\\x99 with â\\x80\\x98extendâ\\x80\\x99, things get much more interesting.\\nTodayâ\\x80\\x99s most successful bot experiences take a hybrid approach, incorporating chat into a broader strategy that encompasses more traditional elements.\\nThe next wave will be multimodal apps, where you can say what you want (like with Siri) and get back information as a map, text, or even a spoken response.\\nAnother problematic aspect of the sweeping nature of hype is that it tends to bypass essential questions like these.\\nFor plenty of companies, bots just arenâ\\x80\\x99t the right solution. The past two years are littered with cases of bots being blindly applied to problems where they arenâ\\x80\\x99t needed.\\nBuilding a bot for the sake of it, letting it loose and hoping for the best will never end well:\\nThe vast majority of bots are built using decision-tree logic, where the botâ\\x80\\x99s canned response relies on spotting specific keywords in the user input.\\nThe advantage of this approach is that itâ\\x80\\x99s pretty easy to list all the cases that they are designed to cover. And thatâ\\x80\\x99s precisely their disadvantage, too.\\nThatâ\\x80\\x99s because these bots are purely a reflection of the capability, fastidiousness and patience of the person who created them; and how many user needs and inputs they were able to anticipate.\\nProblems arise when life refuses to fit into those boxes.\\nAccording to recent reports, 70% of the 100,000+ bots on Facebook Messenger are failing to fulfil simple user requests. This is partly a result of developers failing to narrow their bot down to one strong area of focus.\\nWhen we were building GrowthBot, we decided to make it specific to sales and marketers: not an â\\x80\\x98all-rounderâ\\x80\\x99, despite the temptation to get overexcited about potential capabilties.\\nRemember: a bot that does ONE thing well is infinitely more helpful than a bot that does multiple things poorly.\\nA competent developer can build a basic bot in minutes â\\x80\\x94 but one that can hold a conversation? Thatâ\\x80\\x99s another story. Despite the constant hype around AI, weâ\\x80\\x99re still a long way from achieving anything remotely human-like.\\nIn an ideal world, the technology known as NLP (natural language processing) should allow a chatbot to understand the messages it receives. But NLP is only just emerging from research labs and is very much in its infancy.\\nSome platforms provide a bit of NLP, but even the best is at toddler-level capacity (for example, think about Siri understanding your words, but not their meaning.)\\nAs Matt Asay outlines, this results in another issue: failure to capture the attention and creativity of developers.\\nAnd conversations are complex. Theyâ\\x80\\x99re not linear. Topics spin around each other, take random turns, restart or abruptly finish.\\nTodayâ\\x80\\x99s rule-based dialogue systems are too brittle to deal with this kind of unpredictability, and statistical approaches using machine learning are just as limited. The level of AI required for human-like conversation just isnâ\\x80\\x99t available yet.\\nAnd in the meantime, there are few high-quality examples of trailblazing bots to lead the way. As Dave Feldman remarked:\\nOnce upon a time, the only way to interact with computers was by typing arcane commands to the terminal. Visual interfaces using windows, icons or a mouse were a revolution in how we manipulate information\\nThereâ\\x80\\x99s a reasons computing moved from text-based to graphical user interfaces (GUIs). On the input side, itâ\\x80\\x99s easier and faster to click than it is to type.\\nTapping or selecting is obviously preferable to typing out a whole sentence, even with predictive (often error-prone ) text. On the output side, the old adage that a picture is worth a thousand words is usually true.\\nWe love optical displays of information because we are highly visual creatures. Itâ\\x80\\x99s no accident that kids love touch screens. The pioneers who dreamt up graphical interface were inspired by cognitive psychology, the study of how the brain deals with communication.\\nConversational UIs are meant to replicate the way humans prefer to communicate, but they end up requiring extra cognitive effort. Essentially, weâ\\x80\\x99re swapping something simple for a more-complex alternative.\\nSure, there are some concepts that we can only express using language (â\\x80\\x9cshow me all the ways of getting to a museum that give me 2000 steps but donâ\\x80\\x99t take longer than 35 minutesâ\\x80\\x9d), but most tasks can be carried out more efficiently and intuitively with GUIs than with a conversational UI.\\nAiming for a human dimension in business interactions makes sense.\\nIf thereâ\\x80\\x99s one thing thatâ\\x80\\x99s broken about sales and marketing, itâ\\x80\\x99s the lack of humanity: brands hide behind ticket numbers, feedback forms, do-not-reply-emails, automated responses and gated â\\x80\\x98contact usâ\\x80\\x99 forms.\\nFacebookâ\\x80\\x99s goal is that their bots should pass the so-called Turing Test, meaning you canâ\\x80\\x99t tell whether you are talking to a bot or a human. But a bot isnâ\\x80\\x99t the same as a human. It never will be.\\nA conversation encompasses so much more than just text.\\nHumans can read between the lines, leverage contextual information and understand double layers like sarcasm. Bots quickly forget what theyâ\\x80\\x99re talking about, meaning itâ\\x80\\x99s a bit like conversing with someone who has little or no short-term memory.\\nAs HubSpot team pinpointed:\\nPeople arenâ\\x80\\x99t easily fooled, and pretending a bot is a human is guaranteed to diminish returns (not to mention the fact that youâ\\x80\\x99re lying to your users).\\nAnd even those rare bots that are powered by state-of-the-art NLP, and excel at processing and producing content, will fall short in comparison.\\nAnd hereâ\\x80\\x99s the other thing. Conversational UIs are built to replicate the way humans prefer to communicate â\\x80\\x94 with other humans.\\nBut is that how humans prefer to interact with machines?\\nNot necessarily.\\nAt the end of the day, no amount of witty quips or human-like mannerisms will save a bot from conversational failure.\\nIn a way, those early-adopters werenâ\\x80\\x99t entirely wrong.\\nPeople are yelling at Google Home to play their favorite song, ordering pizza from the Dominoâ\\x80\\x99s bot and getting makeup tips from Sephora. But in terms of consumer response and developer involvement, chatbots havenâ\\x80\\x99t lived up to the hype generated circa 2015/16.\\nNot even close.\\nComputers are good at being computers. Searching for data, crunching numbers, analyzing opinions and condensing that information.\\nComputers arenâ\\x80\\x99t good at understanding human emotion. The state of NLP means they still donâ\\x80\\x99t â\\x80\\x98getâ\\x80\\x99 what weâ\\x80\\x99re asking them, never mind how we feel.\\nThatâ\\x80\\x99s why itâ\\x80\\x99s still impossible to imagine effective customer support, sales or marketing without the essential human touch: empathy and emotional intelligence.\\nFor now, bots can continue to help us with automated, repetitive, low-level tasks and queries; as cogs in a larger, more complex system. And we did them, and ourselves, a disservice by expecting so much, so soon.\\nBut thatâ\\x80\\x99s not the whole story.\\nYes, our industry massively overestimated the initial impact chatbots would have. Emphasis on initial.\\nAs Bill Gates once said:\\nThe hype is over. And thatâ\\x80\\x99s a good thing. Now, we can start examining the middle-grounded grey area, instead of the hyper-inflated, frantic black and white zone.\\nI believe weâ\\x80\\x99re at the very beginning of explosive growth. This sense of anti-climax is completely normal for transformational technology.\\nMessaging will continue to gain traction. Chatbots arenâ\\x80\\x99t going away. NLP and AI are becoming more sophisticated every day.\\nDevelopers, apps and platforms will continue to experiment with, and heavily invest in, conversational marketing.\\nAnd I canâ\\x80\\x99t wait to see what happens next.\\nFrom a quick cheer to a standing ovation, clap to show how much you enjoyed this story.\\nHead of Growth for GrowthBot, Messaging & Conversational Strategy @HubSpot\\nMedium's largest publication for makers. Subscribe to receive our top stories here â\\x86\\x92 https://goo.gl/zHcLJi\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 47
        }
      ],
      "source": [
        "df['text'][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "fb8dadb5",
      "metadata": {
        "id": "fb8dadb5",
        "outputId": "b9ce2e4a-2b7d-4a7f-9411-159f4977a147",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                text\n",
              "0  Oh, how the headlines blared:\\nChatbots were T...\n",
              "1  If youâve ever found yourself looking up the...\n",
              "2  Machine learning is increasingly moving from h...\n",
              "3  If your understanding of A.I. and Machine Lear...\n",
              "4  Want to learn about applied Artificial Intelli..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d70f1921-1a5d-4401-b8c3-6f86a9e3cadc\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Oh, how the headlines blared:\\nChatbots were T...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>If youâve ever found yourself looking up the...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Machine learning is increasingly moving from h...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>If your understanding of A.I. and Machine Lear...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Want to learn about applied Artificial Intelli...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d70f1921-1a5d-4401-b8c3-6f86a9e3cadc')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-d70f1921-1a5d-4401-b8c3-6f86a9e3cadc button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-d70f1921-1a5d-4401-b8c3-6f86a9e3cadc');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-014d7193-b86f-4257-9e47-faa2a9912fa9\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-014d7193-b86f-4257-9e47-faa2a9912fa9')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-014d7193-b86f-4257-9e47-faa2a9912fa9 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 337,\n  \"fields\": [\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 230,\n        \"samples\": [\n          \"There\\u00e2\\u0080\\u0099s a huge difference between reading about Reinforcement Learning and actually implementing it.\\nIn this post, you\\u00e2\\u0080\\u0099ll implement a Neural Network for Reinforcement Learning and see it learn more and more as it finally becomes good enough to beat the computer in Pong! You can play around with other such Atari games at the OpenAI Gym.\\nBy the end of this post, you\\u00e2\\u0080\\u0099ll be able to do the following:\\nThe code and the idea are all tightly based on Andrej Karpathy\\u00e2\\u0080\\u0099s blog post. The code in me_pong.py is intended to be a simpler to follow version of pong.py which was written by Dr. Karpathy.\\nTo follow along, you\\u00e2\\u0080\\u0099ll need to know the following:\\nIf you want a deeper dive into the material at hand, read the blog post on which all of this is based. This post is meant to be a simpler introduction to that material.\\nGreat! Let\\u00e2\\u0080\\u0099s get started.\\nWe are given the following:\\nCan we use these pieces to train our agent to beat the computer? Moreover, can we make our solution generic enough so it can be reused to win in games that aren\\u00e2\\u0080\\u0099t pong?\\nIndeed, we can! Andrej does this by building a Neural Network that takes in each image and outputs a command to our AI to move up or down.\\nWe can break this down a bit more into the following steps:\\nOur Neural Network, based heavily on Andrej\\u00e2\\u0080\\u0099s solution, will do the following:\\nOk now that we\\u00e2\\u0080\\u0099ve described the problem and its solution, let\\u00e2\\u0080\\u0099s get to writing some code!\\nWe\\u00e2\\u0080\\u0099re now going to follow the code in me_pong.py. Please keep it open and read along! The code starts here:\\nFirst, let\\u00e2\\u0080\\u0099s use OpenAI Gym to make a game environment and get our very first image of the game.\\nNext, we set a bunch of parameters based off of Andrej\\u00e2\\u0080\\u0099s blog post. We aren\\u00e2\\u0080\\u0099t going to worry about tuning them but note that you can probably get better performance by doing so. The parameters we will use are:\\nThen, we set counters, initial values, and the initial weights in our Neural Network.\\nWeights are stored in matrices. Layer 1 of our Neural Network is a 200 x 6400 matrix representing the weights for our hidden layer. For layer 1, element w1_ij represents the weight of neuron i for input pixel j in layer 1.\\nLayer 2 is a 200 x 1 matrix representing the weights of the output of the hidden layer on our final output. For layer 2, element w2_i represents the weights we place on the activation of neuron i in the hidden layer.\\nWe initialize each layer\\u00e2\\u0080\\u0099s weights with random numbers for now. We divide by the square root of the number of the dimension size to normalize our weights.\\nNext, we set up the initial parameters for RMSProp (a method for updating weights that we will discuss later). Don\\u00e2\\u0080\\u0099t worry too much about understanding what you see below. I\\u00e2\\u0080\\u0099m mainly bringing it up here so we can continue to follow along the main code block.\\nWe\\u00e2\\u0080\\u0099ll need to collect a bunch of observations and intermediate values across the episode and use those to compute the gradient at the end based on the result. The below sets up the arrays where we\\u00e2\\u0080\\u0099ll collect all that information.\\nOk we\\u00e2\\u0080\\u0099re all done with the setup! If you were following, it should look something like this:\\nPhew. Now for the fun part!\\nThe crux of our algorithm is going to live in a loop where we continually make a move and then learn based on the results of the move. We\\u00e2\\u0080\\u0099ll put everything in a while block for now but in reality you might set up a break condition to stop the process.\\nThe first step to our algorithm is processing the image of the game that OpenAI Gym passed us. We really don\\u00e2\\u0080\\u0099t care about the entire image - just certain details. We do this below:\\nLet\\u00e2\\u0080\\u0099s dive into preprocess_observations to see how we convert the image OpenAI Gym gives us into something we can use to train our Neural Network. The basic steps are:\\nNow that we\\u00e2\\u0080\\u0099ve preprocessed the observations, let\\u00e2\\u0080\\u0099s move on to actually sending the observations through our neural net to generate the probability of telling our AI to move up. Here are the steps we\\u00e2\\u0080\\u0099ll take:\\nHow exactly does apply_neural_nets take observations and weights and generate a probability of going up? This is just the forward pass of the Neural Network. Let\\u00e2\\u0080\\u0099s look at the code below for more information:\\nAs you can see, it\\u00e2\\u0080\\u0099s not many steps at all! Let\\u00e2\\u0080\\u0099s go step by step:\\nLet\\u00e2\\u0080\\u0099s return to the main algorithm and continue on. Now that we have obtained a probability of going up, we need to now record the results for later learning and choose an action to tell our AI to implement:\\nWe choose an action by flipping an imaginary coin that lands \\u00e2\\u0080\\u009cup\\u00e2\\u0080\\u009d with probability up_probability and down with 1 - up_probability. If it lands up, we choose tell our AI to go up and if not, we tell it to go down. We also\\nHaving done that, we pass the action to OpenAI Gym via env.step(action).\\nOk we\\u00e2\\u0080\\u0099ve covered the first half of the solution! We know what action to tell our AI to take. If you\\u00e2\\u0080\\u0099ve been following along, your code should look like this:\\nNow that we\\u00e2\\u0080\\u0099ve made our move, it\\u00e2\\u0080\\u0099s time to start learning so we figure out the right weights in our Neural Network!\\nLearning is all about seeing the result of the action (i.e. whether or not we won the round) and changing our weights accordingly. The first step to learning is asking the following question:\\nMathematically, this is just the derivative of our result with respect to the outputs of our final layer. If L is the value of our result to us and f is the function that gives us the activations of our final layer, this derivative is just \\u00e2\\u0088\\u0082L/\\u00e2\\u0088\\u0082f.\\nIn a binary classification context (i.e. we just have to tell the AI one of two actions, up or down), this derivative turns out to be\\nNote that \\u00cf\\u0083 in the above equation represents the sigmoid function. Read the Attribute Classification section here for more information about how we get the above derivative. We simplify this further below:\\nAfter one action(moving the paddle up or down), we don\\u00e2\\u0080\\u0099t really have an idea of whether or not this was the right action. So we\\u00e2\\u0080\\u0099re going to cheat and treat the action we end up sampling from our probability as the correct action.\\nOur predicion for this round is going to be the probability of going up we calculated. Using that, we have that \\u00e2\\u0088\\u0082L/\\u00e2\\u0088\\u0082f can be computed by\\nAwesome! We have the gradient per action.\\nThe next step is to figure out how we learn after the end of an episode (i.e. when we or our opponent miss the ball and someone gets a point). We do this by computing the policy gradient of the network at the end of each episode. The intuition here is that if we won the round, we\\u00e2\\u0080\\u0099d like our network to generate more of the actions that led to us winning. Alternatively, if we lose, we\\u00e2\\u0080\\u0099re going to try and generate less of these actions.\\nOpenAI Gym provides us the handy done variable to tell us when an episode finishes (i.e. we missed the ball or our opponent missed the ball). When we notice we are done, the first thing we do is compile all our observations and gradient calculations for the episode. This allows us to apply our learnings over all the actions in the episode.\\nNext, we want to learn in such a way that actions taken towards the end of an episode more heavily influence our learning than actions taken at the beginning. This is called discounting.\\nThink about it this way - if you moved up at the first frame of the episode, it probably had very little impact on whether or not you win. However, closer to the end of the episode, your actions probably have a much larger effect as they determine whether or not your paddle reaches the ball and how your paddle hits the ball.\\nWe\\u00e2\\u0080\\u0099re going to take this weighting into account by discounting our rewards such that rewards from earlier frames are discounted a lot more than rewards for later frames. After this, we\\u00e2\\u0080\\u0099re going to finally use backpropagation to compute the gradient (i.e. the direction we need to move our weights to improve).\\nLet\\u00e2\\u0080\\u0099s dig in a bit into how the policy gradient for the episode is computed. This is one of the most important parts of Reinforcement Learning as it\\u00e2\\u0080\\u0099s how our agent figures out how to improve over time.\\nTo begin with, if you haven\\u00e2\\u0080\\u0099t already, read this excerpt on backpropagation from Michael Nielsen\\u00e2\\u0080\\u0099s excellent free book on Deep Learning.\\nAs you\\u00e2\\u0080\\u0099ll see in that excerpt, there are four fundamental equations of backpropogation, a technique for computing the gradient for our weights.\\nOur goal is to find \\u00e2\\u0088\\u0082C/\\u00e2\\u0088\\u0082w1 (BP4), the derivative of the cost function with respect to the first layer\\u00e2\\u0080\\u0099s weights, and \\u00e2\\u0088\\u0082C/\\u00e2\\u0088\\u0082w2, the derivative of the cost function with respect to the second layer\\u00e2\\u0080\\u0099s weights. These gradients will help us understand what direction to move our weights in for the greatest improvement.\\nTo begin with, let\\u00e2\\u0080\\u0099s start with \\u00e2\\u0088\\u0082C/\\u00e2\\u0088\\u0082w2. If a^l2 is the activations of the hidden layer (layer 2), we see that the formula is:\\nIndeed, this is exactly what we do here:\\nNext, we need to calculate \\u00e2\\u0088\\u0082C/\\u00e2\\u0088\\u0082w1. The formula for that is:\\nand we also know that a^l1 is just our observation_values.\\nSo all we need now is \\u00ce\\u00b4^l2. Once we have that, we can calculate \\u00e2\\u0088\\u0082C/\\u00e2\\u0088\\u0082w1 and return. We do just that below:\\nIf you\\u00e2\\u0080\\u0099ve been following along, your function should look like this:\\nWith that, we\\u00e2\\u0080\\u0099ve finished backpropagation and computed our gradients!\\nAfter we have finished batch_size episodes, we finally update our weights for our Neural Network and implement our learnings.\\nTo update the weights, we simply apply RMSProp, an algorithm for updating weights described by Sebastian Reuder here.\\nWe implement this below:\\nThis is the step that tweaks our weights and allows us to get better over time.\\nThis is basically it! Putting it altogether it should look like this.\\nYou just coded a full Neural Network for playing Pong! Uncomment env.render() and run it for 3\\u00e2\\u0080\\u00934 days to see it finally beat the computer! You\\u00e2\\u0080\\u0099ll need to do some pickling as done in Andrej Karpathy\\u00e2\\u0080\\u0099s solution to be able to visualize your results when you win.\\nAccording to the blog post, this algorithm should take around 3 days of training on a Macbook to start beating the computer.\\nConsider tweaking the parameters or using Convolutional Neural Nets to boost the performance further.\\nIf you want a further primer into Neural Networks and Reinforcement Learning, there are some great resources to learn more (I work at Udacity as the Director of Machine Learning programs):\\nFrom a quick cheer to a standing ovation, clap to show how much you enjoyed this story.\\n@dhruvp. VP Eng @Athelas. MIT Math and CS Undergrad \\u00e2\\u0080\\u009913. MIT CS Masters \\u00e2\\u0080\\u009914. Previously: Director of AI Programs @ Udacity.\\n\",\n          \"For a recent hackathon that we did at STATWORX, some of our team members scraped minutely S&P 500 data from the Google Finance API. The data consisted of index as well as stock prices of the S&P\\u00e2\\u0080\\u0099s 500 constituents. Having this data at hand, the idea of developing a deep learning model for predicting the S&P 500 index based on the 500 constituents prices one minute ago came immediately on my mind.\\nPlaying around with the data and building the deep learning model with TensorFlow was fun and so I decided to write my first Medium.com story: a little TensorFlow tutorial on predicting S&P 500 stock prices. What you will read is not an in-depth tutorial, but more a high-level introduction to the important building blocks and concepts of TensorFlow models. The Python code I\\u00e2\\u0080\\u0099ve created is not optimized for efficiency but understandability. The dataset I\\u00e2\\u0080\\u0099ve used can be downloaded from here (40MB).\\nOur team exported the scraped stock data from our scraping server as a csv file. The dataset contains n = 41266 minutes of data ranging from April to August 2017 on 500 stocks as well as the total S&P 500 index price. Index and stocks are arranged in wide format.\\nThe data was already cleaned and prepared, meaning missing stock and index prices were LOCF\\u00e2\\u0080\\u0099ed (last observation carried forward), so that the file did not contain any missing values.\\nA quick look at the S&P time series using pyplot.plot(data['SP500']):\\nNote: This is actually the lead of the S&P 500 index, meaning, its value is shifted 1 minute into the future. This operation is necessary since we want to predict the next minute of the index and not the current minute.\\nThe dataset was split into training and test data. The training data contained 80% of the total dataset. The data was not shuffled but sequentially sliced. The training data ranges from April to approx. end of July 2017, the test data ends end of August 2017.\\nThere are a lot of different approaches to time series cross validation, such as rolling forecasts with and without refitting or more elaborate concepts such as time series bootstrap resampling. The latter involves repeated samples from the remainder of the seasonal decomposition of the time series in order to simulate samples that follow the same seasonal pattern as the original time series but are not exact copies of its values.\\nMost neural network architectures benefit from scaling the inputs (sometimes also the output). Why? Because most common activation functions of the network\\u00e2\\u0080\\u0099s neurons such as tanh or sigmoid are defined on the [-1, 1] or [0, 1] interval respectively. Nowadays, rectified linear unit (ReLU) activations are commonly used activations which are unbounded on the axis of possible activation values. However, we will scale both the inputs and targets anyway. Scaling can be easily accomplished in Python using sklearn\\u00e2\\u0080\\u0099s MinMaxScaler.\\nRemark: Caution must be undertaken regarding what part of the data is scaled and when. A common mistake is to scale the whole dataset before training and test split are being applied. Why is this a mistake? Because scaling invokes the calculation of statistics e.g. the min/max of a variable. When performing time series forecasting in real life, you do not have information from future observations at the time of forecasting. Therefore, calculation of scaling statistics has to be conducted on training data and must then be applied to the test data. Otherwise, you use future information at the time of forecasting which commonly biases forecasting metrics in a positive direction.\\nTensorFlow is a great piece of software and currently the leading deep learning and neural network computation framework. It is based on a C++ low level backend but is usually controlled via Python (there is also a neat TensorFlow library for R, maintained by RStudio). TensorFlow operates on a graph representation of the underlying computational task. This approach allows the user to specify mathematical operations as elements in a graph of data, variables and operators. Since neural networks are actually graphs of data and mathematical operations, TensorFlow is just perfect for neural networks and deep learning. Check out this simple example (stolen from our deep learning introduction from our blog):\\nIn the figure above, two numbers are supposed to be added. Those numbers are stored in two variables, a and b. The two values are flowing through the graph and arrive at the square node, where they are being added. The result of the addition is stored into another variable, c. Actually, a, b and c can be considered as placeholders. Any numbers that are fed into a and b get added and are stored into c. This is exactly how TensorFlow works. The user defines an abstract representation of the model (neural network) through placeholders and variables. Afterwards, the placeholders get \\\"filled\\\" with real data and the actual computations take place. The following code implements the toy example from above in TensorFlow:\\nAfter having imported the TensorFlow library, two placeholders are defined using tf.placeholder(). They correspond to the two blue circles on the left of the image above. Afterwards, the mathematical addition is defined via tf.add(). The result of the computation is c = 9. With placeholders set up, the graph can be executed with any integer value for a and b. Of course, the former problem is just a toy example. The required graphs and computations in a neural network are much more complex.\\nAs mentioned before, it all starts with placeholders. We need two placeholders in order to fit our model: X contains the network's inputs (the stock prices of all S&P 500 constituents at time T = t) and Y the network's outputs (the index value of the S&P 500 at time T = t + 1).\\nThe shape of the placeholders correspond to [None, n_stocks] with [None] meaning that the inputs are a 2-dimensional matrix and the outputs are a 1-dimensional vector. It is crucial to understand which input and output dimensions the neural net needs in order to design it properly.\\nThe None argument indicates that at this point we do not yet know the number of observations that flow through the neural net graph in each batch, so we keep if flexible. We will later define the variable batch_size that controls the number of observations per training batch.\\nBesides placeholders, variables are another cornerstone of the TensorFlow universe. While placeholders are used to store input and target data in the graph, variables are used as flexible containers within the graph that are allowed to change during graph execution. Weights and biases are represented as variables in order to adapt during training. Variables need to be initialized, prior to model training. We will get into that a litte later in more detail.\\nThe model consists of four hidden layers. The first layer contains 1024 neurons, slightly more than double the size of the inputs. Subsequent hidden layers are always half the size of the previous layer, which means 512, 256 and finally 128 neurons. A reduction of the number of neurons for each subsequent layer compresses the information the network identifies in the previous layers. Of course, other network architectures and neuron configurations are possible but are out of scope for this introduction level article.\\nIt is important to understand the required variable dimensions between input, hidden and output layers. As a rule of thumb in multilayer perceptrons (MLPs, the type of networks used here), the second dimension of the previous layer is the first dimension in the current layer for weight matrices. This might sound complicated but is essentially just each layer passing its output as input to the next layer. The biases dimension equals the second dimension of the current layer\\u00e2\\u0080\\u0099s weight matrix, which corresponds the number of neurons in this layer.\\nAfter definition of the required weight and bias variables, the network topology, the architecture of the network, needs to be specified. Hereby, placeholders (data) and variables (weighs and biases) need to be combined into a system of sequential matrix multiplications.\\nFurthermore, the hidden layers of the network are transformed by activation functions. Activation functions are important elements of the network architecture since they introduce non-linearity to the system. There are dozens of possible activation functions out there, one of the most common is the rectified linear unit (ReLU) which will also be used in this model.\\nThe image below illustrates the network architecture. The model consists of three major building blocks. The input layer, the hidden layers and the output layer. This architecture is called a feedforward network. Feedforward indicates that the batch of data solely flows from left to right. Other network architectures, such as recurrent neural networks, also allow data flowing \\u00e2\\u0080\\u009cbackwards\\u00e2\\u0080\\u009d in the network.\\nThe cost function of the network is used to generate a measure of deviation between the network\\u00e2\\u0080\\u0099s predictions and the actual observed training targets. For regression problems, the mean squared error (MSE) function is commonly used. MSE computes the average squared deviation between predictions and targets. Basically, any differentiable function can be implemented in order to compute a deviation measure between predictions and targets.\\nHowever, the MSE exhibits certain properties that are advantageous for the general optimization problem to be solved.\\nThe optimizer takes care of the necessary computations that are used to adapt the network\\u00e2\\u0080\\u0099s weight and bias variables during training. Those computations invoke the calculation of so called gradients, that indicate the direction in which the weights and biases have to be changed during training in order to minimize the network\\u00e2\\u0080\\u0099s cost function. The development of stable and speedy optimizers is a major field in neural network an deep learning research.\\nHere the Adam Optimizer is used, which is one of the current default optimizers in deep learning development. Adam stands for \\u00e2\\u0080\\u009cAdaptive Moment Estimation\\u00e2\\u0080\\u009d and can be considered as a combination between two other popular optimizers AdaGrad and RMSProp.\\nInitializers are used to initialize the network\\u00e2\\u0080\\u0099s variables before training. Since neural networks are trained using numerical optimization techniques, the starting point of the optimization problem is one the key factors to find good solutions to the underlying problem. There are different initializers available in TensorFlow, each with different initialization approaches. Here, I use the tf.variance_scaling_initializer(), which is one of the default initialization strategies.\\nNote, that with TensorFlow it is possible to define multiple initialization functions for different variables within the graph. However, in most cases, a unified initialization is sufficient.\\nAfter having defined the placeholders, variables, initializers, cost functions and optimizers of the network, the model needs to be trained. Usually, this is done by minibatch training. During minibatch training random data samples of n = batch_size are drawn from the training data and fed into the network. The training dataset gets divided into n / batch_size batches that are sequentially fed into the network. At this point the placeholders X and Y come into play. They store the input and target data and present them to the network as inputs and targets.\\nA sampled data batch of X flows through the network until it reaches the output layer. There, TensorFlow compares the models predictions against the actual observed targets Y in the current batch. Afterwards, TensorFlow conducts an optimization step and updates the networks parameters, corresponding to the selected learning scheme. After having updated the weights and biases, the next batch is sampled and the process repeats itself. The procedure continues until all batches have been presented to the network. One full sweep over all batches is called an epoch.\\nThe training of the network stops once the maximum number of epochs is reached or another stopping criterion defined by the user applies.\\nDuring the training, we evaluate the networks predictions on the test set \\u00e2\\u0080\\u0094 the data which is not learned, but set aside \\u00e2\\u0080\\u0094 for every 5th batch and visualize it. Additionally, the images are exported to disk and later combined into a video animation of the training process (see below). The model quickly learns the shape und location of the time series in the test data and is able to produce an accurate prediction after some epochs. Nice!\\nOne can see that the networks rapidly adapts to the basic shape of the time series and continues to learn finer patterns of the data. This also corresponds to the Adam learning scheme that lowers the learning rate during model training in order not to overshoot the optimization minimum. After 10 epochs, we have a pretty close fit to the test data! The final test MSE equals 0.00078 (it is very low, because the target is scaled). The mean absolute percentage error of the forecast on the test set is equal to 5.31% which is pretty good. Note, that this is just a fit to the test data, no actual out of sample metrics in a real world scenario.\\nPlease note that there are tons of ways of further improving this result: design of layers and neurons, choosing different initialization and activation schemes, introduction of dropout layers of neurons, early stopping and so on. Furthermore, different types of deep learning models, such as recurrent neural networks might achieve better performance on this task. However, this is not the scope of this introductory post.\\nThe release of TensorFlow was a landmark event in deep learning research. Its flexibility and performance allows researchers to develop all kinds of sophisticated neural network architectures as well as other ML algorithms. However, flexibility comes at the cost of longer time-to-model cycles compared to higher level APIs such as Keras or MxNet. Nonetheless, I am sure that TensorFlow will make its way to the de-facto standard in neural network and deep learning development in research and practical applications. Many of our customers are already using TensorFlow or start developing projects that employ TensorFlow models. Also our data science consultants at STATWORX are heavily using TensorFlow for deep learning and neural net research and development. Let\\u00e2\\u0080\\u0099s see what Google has planned for the future of TensorFlow. One thing that is missing, at least in my opinion, is a neat graphical user interface for designing and developing neural net architectures with TensorFlow backend. Maybe, this is something Google is already working on ;)\\nIf you have any comments or questions on my first Medium story, feel free to comment below! I will try to answer them. Also, feel free to use my code or share this story with your peers on social platforms of your choice.\\nUpdate: I\\u00e2\\u0080\\u0099ve added both the Python script as well as a (zipped) dataset to a Github repository. Feel free to clone and fork.\\nLastly, follow me on: Twitter | LinkedIn\\nFrom a quick cheer to a standing ovation, clap to show how much you enjoyed this story.\\nCEO @ STATWORX. Doing data science, stats and ML for over a decade. Food, wine and cocktail enthusiast. Check our website: https://www.statworx.com\\nHighlights from Machine Learning Research, Projects and Learning Materials. From and For ML Scientists, Engineers an Enthusiasts.\\n\",\n          \"Over the past 8 months, I\\u00e2\\u0080\\u0099ve been interviewing at various companies like Google\\u00e2\\u0080\\u0099s DeepMind, Wadhwani Institute of AI, Microsoft, Ola, Fractal Analytics, and a few others primarily for the roles \\u00e2\\u0080\\u0094 Data Scientist, Software Engineer & Research Engineer. In the process, not only did I get an opportunity to interact with many great minds, but also had a peek at myself along with a sense of what people really look for when interviewing someone. I believe that if I\\u00e2\\u0080\\u0099d had this knowledge before, I could have avoided many mistakes and have prepared in a much better manner, which is what the motivation behind this post is, to be able to help someone bag their dream place of work.\\nThis post arose from a discussion with one of my juniors on the lack of really fulfilling job opportunities offered through campus placements for people working in AI. Also, when I was preparing, I noticed people using a lot of resources but as per my experience over the past months, I realised that one can do away with a few minimal ones for most roles in AI, all of which I\\u00e2\\u0080\\u0099m going to mention at the end of the post. I begin with How to get noticed a.k.a. the interview. Then I provide a List of companies and start-ups to apply, which is followed by How to ace that interview. Based on whatever experience I\\u00e2\\u0080\\u0099ve had, I add a section on What we should strive to work for. I conclude with Minimal Resources you need for preparation.\\nNOTE: For people who are sitting for campus placements, there are two things I\\u00e2\\u0080\\u0099d like to add. Firstly, most of what I\\u00e2\\u0080\\u0099m going to say (except for the last one maybe) is not going to be relevant to you for placements. But, and this is my second point, as I mentioned before, opportunities on campus are mostly in software engineering roles having no intersection with AI. So, this post is specifically meant for people who want to work on solving interesting problems using AI. Also, I want to add that I haven\\u00e2\\u0080\\u0099t cleared all of these interviews but I guess that\\u00e2\\u0080\\u0099s the essence of failure \\u00e2\\u0080\\u0094 it\\u00e2\\u0080\\u0099s the greatest teacher! The things that I mention here may not all be useful but these are things that I did and there\\u00e2\\u0080\\u0099s no way for me to know what might have ended up making my case stronger.\\nTo be honest, this step is the most important one. What makes off-campus placements so tough and exhausting is getting the recruiter to actually go through your profile among the plethora of applications that they get. Having a contact inside the organisation place a referral for you would make it quite easy, but, in general, this part can be sub-divided into three keys steps:\\na) Do the regulatory preparation and do that well: So, with regulatory preparation, I mean \\u00e2\\u0080\\u0094a LinkedIn profile, a Github profile, a portfolio website and a well-polished CV. Firstly, your CV should be really neat and concise. Follow this guide by Udacity for cleaning up your CV \\u00e2\\u0080\\u0094 Resume Revamp. It has everything that I intend to say and I\\u00e2\\u0080\\u0099ve been using it as a reference guide myself. As for the CV template, some of the in-built formats on Overleaf are quite nice. I personally use deedy-resume. Here\\u00e2\\u0080\\u0099s a preview:\\nAs it can be seen, a lot of content can be fit into one page. However, if you really do need more than that, then the format linked above would not work directly. Instead, you can find a modified multi-page format of the same here. The next most important thing to mention is your Github profile. A lot of people underestimate the potential of this, just because unlike LinkedIn, it doesn\\u00e2\\u0080\\u0099t have a \\u00e2\\u0080\\u009cWho Viewed Your Profile\\u00e2\\u0080\\u009d option. People DO go through your Github because that\\u00e2\\u0080\\u0099s the only way they have to validate what you have mentioned in your CV, given that there\\u00e2\\u0080\\u0099s a lot of noise today with people associating all kinds of buzzwords with their profile. Especially for data science, open-source has a big role to play too with majority of the tools, implementations of various algorithms, lists of learning resources, all being open-sourced. I discuss the benefits of getting involved in Open-Source and how one can start from scratch in an earlier post here. The bare minimum for now should be:\\n\\u00e2\\u0080\\u00a2 Create a Github account if you don\\u00e2\\u0080\\u0099t already have one.\\u00e2\\u0080\\u00a2 Create a repository for each of the projects that you have done.\\u00e2\\u0080\\u00a2 Add documentation with clear instructions on how to run the code\\u00e2\\u0080\\u00a2 Add documentation for each file mentioning the role of each function, the meaning of each parameter, proper formatting (e.g. PEP8 for Python) along with a script to automate the previous step (Optional).\\nMoving on, the third step is what most people lack, which is having a portfolio website demonstrating their experience and personal projects. Making a portfolio indicates that you are really serious about getting into the field and adds a lot of points to the authenticity factor. Also, you generally have space constraints on your CV and tend to miss out on a lot of details. You can use your portfolio to really delve deep into the details if you want to and it\\u00e2\\u0080\\u0099s highly recommended to include some sort of visualisation or demonstration of the project/idea. It\\u00e2\\u0080\\u0099s really easy to create one too as there are a lot of free platforms with drag-and-drop features making the process really painless. I personally use Weebly which is a widely used tool. It\\u00e2\\u0080\\u0099s better to have a reference to begin with. There are a lot of awesome ones out there but I referred to Deshraj Yadav\\u00e2\\u0080\\u0099s personal website to begin with making mine:\\nFinally, a lot of recruiters and start-ups have nowadays started using LinkedIn as their go-to platform for hiring. A lot of good jobs get posted there. Apart from recruiters, the people working at influential positions are quite active there as well. So, if you can grab their attention, you have a good chance of getting in too. Apart from that, maintaining a clean profile is necessary for people to have the will to connect with you. An important part of LinkedIn is their search tool and for you to show up, you must have the relevant keywords interspersed over your profile. It took me a lot of iterations and re-evaluations to finally have a decent one. Also, you should definitely ask people with or under whom you\\u00e2\\u0080\\u0099ve worked with to endorse you for your skills and add a recommendation talking about their experience of working with you. All of this increases your chance of actually getting noticed. I\\u00e2\\u0080\\u0099ll again point towards Udacity\\u00e2\\u0080\\u0099s guide for LinkedIn and Github profiles.\\nAll this might seem like a lot, but remember that you don\\u00e2\\u0080\\u0099t need to do it in a single day or even a week or a month. It\\u00e2\\u0080\\u0099s a process, it never ends. Setting up everything at first would definitely take some effort but once it\\u00e2\\u0080\\u0099s there and you keep updating it regularly as events around you keep happening, you\\u00e2\\u0080\\u0099ll not only find it to be quite easy, but also you\\u00e2\\u0080\\u0099ll be able to talk about yourself anywhere anytime without having to explicitly prepare for it because you become so aware about yourself.\\nb) Stay authentic: I\\u00e2\\u0080\\u0099ve seen a lot of people do this mistake of presenting themselves as per different job profiles. According to me, it\\u00e2\\u0080\\u0099s always better to first decide what actually interests you, what would you be happy doing and then search for relevant opportunities; not the other way round. The fact that the demand for AI talent surpasses the supply for the same gives you this opportunity. Spending time on your regulatory preparation mentioned above would give you an all-around perspective on yourself and help make this decision easier. Also, you won\\u00e2\\u0080\\u0099t need to prepare answers to various kinds of questions that you get asked during an interview. Most of them would come out naturally as you\\u00e2\\u0080\\u0099d be talking about something you really care about.\\nc) Networking: Once you\\u00e2\\u0080\\u0099re done with a), figured out b), Networking is what will actually help you get there. If you don\\u00e2\\u0080\\u0099t talk to people, you miss out on hearing about many opportunities that you might have a good shot at. It\\u00e2\\u0080\\u0099s important to keep connecting with new people each day, if not physically, then on LinkedIn, so that upon compounding it after many days, you have a large and strong network. Networking is NOT messaging people to place a referral for you. When I was starting off, I did this mistake way too often until I stumbled upon this excellent article by Mark Meloon, where he talks about the importance of building a real connection with people by offering our help first. Another important step in networking is to get your content out. For example, if you\\u00e2\\u0080\\u0099re good at something, blog about it and share that blog on Facebook and LinkedIn. Not only does this help others, it helps you as well. Once you have a good enough network, your visibility increases multi-fold. You never know how one person from your network liking or commenting on your posts, may help you reach out to a much broader audience including people who might be looking for someone of your expertise.\\nI\\u00e2\\u0080\\u0099m presenting this list in alphabetical order to avoid the misinterpretation of any specific preference. However, I do place a \\u00e2\\u0080\\u009c*\\u00e2\\u0080\\u009d on the ones that I\\u00e2\\u0080\\u0099d personally recommend. This recommendation is based on either of the following: mission statement, people, personal interaction or scope of learning. More than 1 \\u00e2\\u0080\\u009c*\\u00e2\\u0080\\u009d is purely based on the 2nd and 3rd factors.\\nYour interview begins the moment you have entered the room and a lot of things can happen between that moment and the time when you\\u00e2\\u0080\\u0099re asked to introduce yourself \\u00e2\\u0080\\u0094 your body language and the fact that you\\u00e2\\u0080\\u0099re smiling while greeting them plays a big role, especially when you\\u00e2\\u0080\\u0099re interviewing for a start-up as culture-fit is something that they extremely care about. You need to understand that as much as the interviewer is a stranger to you, you\\u00e2\\u0080\\u0099re a stranger to him/her too. So, they\\u00e2\\u0080\\u0099re probably just as nervous as you are.\\nIt\\u00e2\\u0080\\u0099s important to view the interview as more of a conversation between yourself and the interviewer. Both of you are looking for a mutual fit \\u00e2\\u0080\\u0094 you are looking for an awesome place to work at and the interviewer is looking for an awesome person (like you) to work with. So, make sure that you\\u00e2\\u0080\\u0099re feeling good about yourself and that you take the charge of making the initial moments of your conversation pleasant for them. And the easiest way I know how to make that happen is to smile.\\nThere are mostly two types of interviews \\u00e2\\u0080\\u0094 one, where the interviewer has come with come prepared set of questions and is going to just ask you just that irrespective of your profile and the second, where the interview is based on your CV. I\\u00e2\\u0080\\u0099ll start with the second one.\\nThis kind of interview generally begins with a \\u00e2\\u0080\\u009cCan you tell me a bit about yourself?\\u00e2\\u0080\\u009d. At this point, 2 things are a big NO \\u00e2\\u0080\\u0094 talking about your GPA in college and talking about your projects in detail. An ideal statement should be about a minute or two long, should give a good idea on what have you been doing till now, and it\\u00e2\\u0080\\u0099s not restricted to academics. You can talk about your hobbies like reading books, playing sports, meditation, etc \\u00e2\\u0080\\u0094 basically, anything that contributes to defining you. The interviewer will then take something that you talk about here as a cue for his next question, and then the technical part of the interview begins. The motive of this kind of interview is to really check whether whatever you have written on your CV is true or not:\\nThere would be a lot of questions on what could be done differently or if \\u00e2\\u0080\\u009cX\\u00e2\\u0080\\u009d was used instead of \\u00e2\\u0080\\u009cY\\u00e2\\u0080\\u009d, what would have happened. At this point, it\\u00e2\\u0080\\u0099s important to know the kind of trade-offs that is usually made during implementation, for e.g. if the interviewer says that using a more complex model would have given better results, then you might say that you actually had less data to work with and that would have lead to overfitting. In one of the interviews, I was given a case-study to work on and it involved designing algorithms for a real-world use case. I\\u00e2\\u0080\\u0099ve noticed that once I\\u00e2\\u0080\\u0099ve been given the green flag to talk about a project, the interviewers really like it when I talk about it in the following flow:\\nProblem > 1 or 2 previous approaches > Our approach > Result > Intuition\\nThe other kind of interview is really just to test your basic knowledge. Don\\u00e2\\u0080\\u0099t expect those questions to be too hard. But they would definitely scratch every bit of the basics that you should be having, mainly based around Linear Algebra, Probability, Statistics, Optimisation, Machine Learning and/or Deep Learning. The resources mentioned in the Minimal Resources you need for preparation section should suffice, but make sure that you don\\u00e2\\u0080\\u0099t miss out one bit among them. The catch here is the amount of time you take to answer those questions. Since these cover the basics, they expect that you should be answering them almost instantly. So, do your preparation accordingly.\\nThroughout the process, it\\u00e2\\u0080\\u0099s important to be confident and honest about what you know and what you don\\u00e2\\u0080\\u0099t know. If there\\u00e2\\u0080\\u0099s a question that you\\u00e2\\u0080\\u0099re certain you have no idea about, say it upfront rather than making \\u00e2\\u0080\\u009cAah\\u00e2\\u0080\\u009d, \\u00e2\\u0080\\u009cUm\\u00e2\\u0080\\u009d sounds. If some concept is really important but you are struggling with answering it, the interviewer would generally (depending on how you did in the initial parts) be happy to give you a hint or guide you towards the right solution. It\\u00e2\\u0080\\u0099s a big plus if you manage to pick their hints and arrive at the correct solution. Try to not get nervous and the best way to avoid that is by, again, smiling.\\nNow we come to the conclusion of the interview where the interviewer would ask you if you have any questions for them. It\\u00e2\\u0080\\u0099s really easy to think that your interview is done and just say that you have nothing to ask. I know many people who got rejected just because of failing at this last question. As I mentioned before, it\\u00e2\\u0080\\u0099s not only you who is being interviewed. You are also looking for a mutual fit with the company itself. So, it\\u00e2\\u0080\\u0099s quite obvious that if you really want to join a place, you must have many questions regarding the work culture there or what kind of role are they seeing you in. It can be as simple as being curious about the person interviewing you. There\\u00e2\\u0080\\u0099s always something to learn from everything around you and you should make sure that you leave the interviewer with the impression that you\\u00e2\\u0080\\u0099re truly interested in being a part of their team. A final question that I\\u00e2\\u0080\\u0099ve started asking all my interviewers, is for a feedback on what they might want me to improve on. This has helped me tremendously and I still remember every feedback that I\\u00e2\\u0080\\u0099ve gotten which I\\u00e2\\u0080\\u0099ve incorporated into my daily life.\\nThat\\u00e2\\u0080\\u0099s it. Based on my experience, if you\\u00e2\\u0080\\u0099re just honest about yourself, are competent, truly care about the company you\\u00e2\\u0080\\u0099re interviewing for and have the right mindset, you should have ticked all the right boxes and should be getting a congratulatory mail soon \\u00f0\\u009f\\u0098\\u0084\\nWe live in an era full of opportunities and that applies to anything that you love. You just need to strive to become the best at it and you will find a way to monetise it. As Gary Vaynerchuk (just follow him already) says:\\nThis is a great time to be working in AI and if you\\u00e2\\u0080\\u0099re truly passionate about it, you have so much that you can do with AI. You can empower so many people that have always been under-represented. We keep nagging about the problems surrounding us, but there\\u00e2\\u0080\\u0099s been never such a time where common people like us can actually do something about those problems, rather than just complaining. Jeffrey Hammerbacher (Founder, Cloudera) had famously said:\\nWe can do so much with AI than we can ever imagine. There are many extremely challenging problems out there which require incredibly smart people like you to put your head down on and solve. You can make many lives better. Time to let go of what is \\u00e2\\u0080\\u009ccool\\u00e2\\u0080\\u009d, or what would \\u00e2\\u0080\\u009clook good\\u00e2\\u0080\\u009d. THINK and CHOOSE wisely.\\nAny Data Science interview comprises of questions mostly of a subset of the following four categories: Computer Science, Math, Statistics and Machine Learning.\\nIf you\\u00e2\\u0080\\u0099re not familiar with the math behind Deep Learning, then you should consider going over my last post for resources to understand them. However, if you are comfortable, I\\u00e2\\u0080\\u0099ve found that the chapters 2, 3 and 4 of the Deep Learning Book are enough to prepare/revise for theoretical questions during such interviews. I\\u00e2\\u0080\\u0099ve been preparing summaries for a few chapters which you can refer to where I\\u00e2\\u0080\\u0099ve tried to even explain a few concepts that I found challenging to understand at first, in case you are not willing to go through the entire chapters. And if you\\u00e2\\u0080\\u0099ve already done a course on probability, you should be comfortable answering a few numerical as well. For stats, covering these topics should be enough.\\nNow, the range of questions here can vary depending on the type of position you are applying for. If it\\u00e2\\u0080\\u0099s a more traditional Machine Learning based interview where they want to check your basic knowledge in ML, you can complete any one of the following courses:- Machine Learning by Andrew Ng \\u00e2\\u0080\\u0094 CS 229- Machine Learning course by Caltech Professor Yaser Abu-Mostafa\\nImportant topics are: Supervised Learning (Classification, Regression, SVM, Decision Tree, Random Forests, Logistic Regression, Multi-layer Perceptron, Parameter Estimation, Bayes\\u00e2\\u0080\\u0099 Decision Rule), Unsupervised Learning (K-means Clustering, Gaussian Mixture Models), Dimensionality Reduction (PCA).\\nNow, if you\\u00e2\\u0080\\u0099re applying for a more advanced position, there\\u00e2\\u0080\\u0099s a high chance that you might be questioned on Deep Learning. In that case, you should be very comfortable with Convolutional Neural Networks (CNNs) and/or (depending upon what you\\u00e2\\u0080\\u0099ve worked on) Recurrent Neural Networks (RNNs) and their variants. And by being comfortable, you must know what is the fundamental idea behind Deep Learning, how CNNs/RNNs actually worked, what kind of architectures have been proposed and what has been the motivation behind those architectural changes. Now, there\\u00e2\\u0080\\u0099s no shortcut for this. Either you understand them or you put enough time to understand them. For CNNs, the recommended resource is Stanford\\u00e2\\u0080\\u0099s CS 231N and CS 224N for RNNs. I found this Neural Network class by Hugo Larochelle to be really enlightening too. Refer this for a quick refresher too. Udacity coming to the aid here too. By now, you should have figured out that Udacity is a really important place for an ML practitioner. There are not a lot of places working on Reinforcement Learning (RL) in India and I too am not experienced in RL as of now. So, that\\u00e2\\u0080\\u0099s one thing to add to this post sometime in the future.\\nGetting placed off-campus is a long journey of self-realisation. I realise that this has been another long post and I\\u00e2\\u0080\\u0099m again extremely grateful to you for valuing my thoughts. I hope that this post finds a way of being useful to you and that it helped you in some way to prepare for your next Data Science interview better. If it did, I request you to really think about what I talk about in What we should strive to work for.\\nI\\u00e2\\u0080\\u0099m very thankful to my friends from IIT Guwahati for their helpful feedback, especially Ameya Godbole, Kothapalli Vignesh and Prabal Jain. A majority of what I mention here, like \\u00e2\\u0080\\u009cviewing an interview as a conversation\\u00e2\\u0080\\u009d and \\u00e2\\u0080\\u009cseeking feedback from our interviewers\\u00e2\\u0080\\u009d, arose from multiple discussions with Prabal who has been advising me constantly on how I can improve my interviewing skills.\\nThis story is published in Noteworthy, where thousands come every day to learn about the people & ideas shaping the products we love.\\nFollow our publication to see more product & design stories featured by the Journal team.\\nFrom a quick cheer to a standing ovation, clap to show how much you enjoyed this story.\\nAI Fanatic \\u00e2\\u0080\\u00a2 Math Lover \\u00e2\\u0080\\u00a2 Dreamer\\nThe official Journal blog\\n\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 48
        }
      ],
      "source": [
        "df = df[[\"text\"]]\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.head(50)"
      ],
      "metadata": {
        "id": "c5R4IBqofwkl"
      },
      "id": "c5R4IBqofwkl",
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "23521b74",
      "metadata": {
        "id": "23521b74",
        "outputId": "5c3fe3fa-60de-4879-a588-c48cb5e0944d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No.of articles :  50\n"
          ]
        }
      ],
      "source": [
        "print(\"No.of articles : \",len(df['text'].values))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "0b420199",
      "metadata": {
        "id": "0b420199"
      },
      "outputs": [],
      "source": [
        "data = df['text'].values"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "077c56c8",
      "metadata": {
        "id": "077c56c8"
      },
      "source": [
        "### Preprocessing Steps"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "005418eb",
      "metadata": {
        "id": "005418eb"
      },
      "source": [
        "1. Remove HTML tags </br>\n",
        "2. Removing Punctuation </br>\n",
        "3. Removing special numbers or digits </br>\n",
        "4. Removing special characters </br>\n",
        "5. Removing extra whitespaces </br>\n",
        "6. Removing stopwords </br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "dc9e6f97",
      "metadata": {
        "id": "dc9e6f97",
        "outputId": "ef05ad7c-4a7c-4bdf-ec14-d8c6c1447740",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ],
      "source": [
        "from string import punctuation\n",
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "id": "f580c116",
      "metadata": {
        "id": "f580c116"
      },
      "outputs": [],
      "source": [
        "remov = list(stopwords.words('english')) + list(punctuation)\n",
        "remov.remove('not')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8810734d",
      "metadata": {
        "id": "8810734d"
      },
      "source": [
        "def preprocess_text(text):\n",
        "    # Convert text to lowercase\n",
        "    text = text.lower()\n",
        "    \n",
        "    # Split text into sentences\n",
        "    sentences = text.split('\\n')\n",
        "    \n",
        "    # Initialize list to store preprocessed sentences\n",
        "    preprocessed_sentences = []\n",
        "    \n",
        "    # Define regex pattern to remove special characters and punctuation\n",
        "    pattern = r'[^a-zA-Z\\s]'\n",
        "    \n",
        "    # Remove special characters, punctuation, and stopwords, and tokenize sentences\n",
        "    for sentence in sentences:\n",
        "        # Remove special characters and punctuation\n",
        "        sentence = re.sub(pattern, '', sentence)\n",
        "        \n",
        "        # Tokenize sentence\n",
        "        tokens = word_tokenize(sentence)\n",
        "        \n",
        "        # Remove stopwords\n",
        "        stop_words = set(stopwords.words('english'))\n",
        "        #tokens = [word for word in tokens if word not in stop_words]\n",
        "        tokens = [word for word in tokens]\n",
        "        \n",
        "        # Add preprocessed sentence to list\n",
        "        preprocessed_sentences.append(tokens)\n",
        "    \n",
        "    return preprocessed_sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "7b7dad10",
      "metadata": {
        "id": "7b7dad10"
      },
      "outputs": [],
      "source": [
        "def preprocess_text(text):\n",
        "    # Convert text to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Split text into sentences\n",
        "    sentences = text.split('\\n')\n",
        "\n",
        "    # Initialize list to store preprocessed sentences\n",
        "    preprocessed_sentences = []\n",
        "\n",
        "    # Define regex pattern to remove special characters and punctuation\n",
        "    pattern = r'[^a-zA-Z\\s]'\n",
        "\n",
        "    # Remove special characters, punctuation, and stopwords, and tokenize sentences\n",
        "    for sentence in sentences:\n",
        "        # Remove special characters and punctuation\n",
        "        sentence = re.sub(pattern, '', sentence)\n",
        "\n",
        "        # Tokenize sentence\n",
        "        tokens = word_tokenize(sentence)\n",
        "\n",
        "        # Remove stopwords\n",
        "        stop_words = set(stopwords.words('english'))\n",
        "        tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "        # Join tokens into a single string\n",
        "        preprocessed_sentence = ' '.join(tokens)\n",
        "\n",
        "        # Add preprocessed sentence to list\n",
        "        preprocessed_sentences.append(preprocessed_sentence)\n",
        "\n",
        "    return preprocessed_sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "id": "b72d4f59",
      "metadata": {
        "id": "b72d4f59"
      },
      "outputs": [],
      "source": [
        "# Preprocess text column\n",
        "corpus = []\n",
        "for text in df['text']:\n",
        "    preprocessed_sentences = preprocess_text(text)\n",
        "    corpus.extend(preprocessed_sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "id": "bed61a42",
      "metadata": {
        "id": "bed61a42"
      },
      "outputs": [],
      "source": [
        "# Print preprocessed corpus\n",
        "#print(corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "id": "2df4383b",
      "metadata": {
        "id": "2df4383b",
        "outputId": "a75c86fb-fc57-40c0-f608-02e5f0f4de91",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Sentences in Corpus :  2063\n"
          ]
        }
      ],
      "source": [
        "print(\"Number of Sentences in Corpus : \", len(corpus))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "id": "6fb72371",
      "metadata": {
        "id": "6fb72371",
        "outputId": "edbcfe07-b58e-4125-ef59-1b64f16a0dcf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'oh headlines blared'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 58
        }
      ],
      "source": [
        "corpus[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "id": "70a36a78",
      "metadata": {
        "id": "70a36a78",
        "outputId": "a830fc9d-748c-42fe-8294-a84edf9803ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'chatbots next big thing'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 59
        }
      ],
      "source": [
        "corpus[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "id": "f34b7c93",
      "metadata": {
        "id": "f34b7c93",
        "outputId": "afd08113-922e-4f82-e74c-75be1b969953",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['oh headlines blared',\n",
              " 'chatbots next big thing',\n",
              " 'hopes sky high brighteyed bushytailed industry ripe new era innovation time start socializing machines']"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ],
      "source": [
        "corpus[0:3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "id": "cb9d424c",
      "metadata": {
        "id": "cb9d424c"
      },
      "outputs": [],
      "source": [
        "tokenizer = Tokenizer() #instantiating the tokenizer\n",
        "tokenizer.fit_on_texts(corpus) #creates tokens for each words\n",
        "total_words = len(tokenizer.word_index) + 1 #calculating total number of words in the initial sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "id": "5faabaae",
      "metadata": {
        "id": "5faabaae",
        "outputId": "fbd3da84-0ea0-42b3-c392-d663f31b251e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8039"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ],
      "source": [
        "total_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "id": "34282770",
      "metadata": {
        "id": "34282770"
      },
      "outputs": [],
      "source": [
        "#tokenizer.word_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "id": "c92da731",
      "metadata": {
        "id": "c92da731",
        "outputId": "cda685c5-3be8-4cfc-cbd6-a7f41d5c2e60",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2552, 4428, 4429]\n"
          ]
        }
      ],
      "source": [
        "for line in corpus:\n",
        "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "    print(token_list)\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "id": "57636467",
      "metadata": {
        "id": "57636467"
      },
      "outputs": [],
      "source": [
        "input_sequences = [] #training features (x) will be a list\n",
        "\n",
        "for line in corpus:\n",
        "    #converts each sentence as its tokenized equivalent\n",
        "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "    for i in range(1, len(token_list)):\n",
        "        #generating n gram sequences\n",
        "        n_gram_sequence = token_list[:i+1]\n",
        "        #appending each n gram sequence to the list of our features (xs)\n",
        "        input_sequences.append(n_gram_sequence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "id": "83626ca9",
      "metadata": {
        "id": "83626ca9",
        "outputId": "bd3be125-1a62-47f9-fe5e-c68e94f53538",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2552, 4428]"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ],
      "source": [
        "input_sequences[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "id": "3ebd9462",
      "metadata": {
        "id": "3ebd9462",
        "outputId": "f0cc5d5f-e42b-43d1-cdaa-57488a7adf50",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2552, 4428, 4429]"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ],
      "source": [
        "input_sequences[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "id": "30f92a42",
      "metadata": {
        "id": "30f92a42",
        "outputId": "ef0a28e1-95e7-4c11-f8c0-69a561b1a118",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[904, 93]"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ],
      "source": [
        "input_sequences[2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "id": "27a257b0",
      "metadata": {
        "id": "27a257b0"
      },
      "outputs": [],
      "source": [
        "#calculating the length of the longest sequence\n",
        "max_sequence_len = max([len(x) for x in input_sequences])\n",
        "#pre-pading each value of the input_sequence\n",
        "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
        "#creating xs and their labels using numpy slicing\n",
        "xs, labels = input_sequences[:,:-1],input_sequences[:,-1]\n",
        "#creating one hot encoding values\n",
        "ys = tf.keras.utils.to_categorical(labels, num_classes=total_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "id": "d50ffdb0",
      "metadata": {
        "id": "d50ffdb0"
      },
      "outputs": [],
      "source": [
        "#creating a sequential model\n",
        "model = Sequential()\n",
        "#adding an embedding layer with 64 as the embedding dimension\n",
        "model.add(Embedding(total_words, 64, input_length=max_sequence_len-1))\n",
        "#adding 20 LSTM units\n",
        "model.add(Bidirectional(LSTM(20)))\n",
        "#creating a dense layer with 54 output units (total_words) with softmax activation\n",
        "model.add(Dense(total_words, activation='softmax'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "id": "6dc30d78",
      "metadata": {
        "id": "6dc30d78"
      },
      "outputs": [],
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='adam',\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "id": "78a60733",
      "metadata": {
        "id": "78a60733",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d48ed96-af7f-4c32-b574-21c9afe92a27"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, 221, 64)           514496    \n",
            "                                                                 \n",
            " bidirectional_1 (Bidirecti  (None, 40)                13600     \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 8039)              329599    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 857695 (3.27 MB)\n",
            "Trainable params: 857695 (3.27 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "id": "a8f74b69",
      "metadata": {
        "id": "a8f74b69",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc95ba68-3b4a-4175-f597-3686a88810cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "1515/1515 [==============================] - 98s 63ms/step - loss: 8.1835 - accuracy: 0.0157\n",
            "Epoch 2/200\n",
            "1515/1515 [==============================] - 39s 26ms/step - loss: 7.8099 - accuracy: 0.0204\n",
            "Epoch 3/200\n",
            "1515/1515 [==============================] - 36s 24ms/step - loss: 7.6661 - accuracy: 0.0209\n",
            "Epoch 4/200\n",
            "1515/1515 [==============================] - 35s 23ms/step - loss: 7.5161 - accuracy: 0.0245\n",
            "Epoch 5/200\n",
            "1515/1515 [==============================] - 34s 22ms/step - loss: 7.3535 - accuracy: 0.0287\n",
            "Epoch 6/200\n",
            "1515/1515 [==============================] - 34s 22ms/step - loss: 7.1850 - accuracy: 0.0354\n",
            "Epoch 7/200\n",
            "1515/1515 [==============================] - 33s 22ms/step - loss: 7.0202 - accuracy: 0.0407\n",
            "Epoch 8/200\n",
            "1515/1515 [==============================] - 32s 21ms/step - loss: 6.8670 - accuracy: 0.0462\n",
            "Epoch 9/200\n",
            "1515/1515 [==============================] - 31s 21ms/step - loss: 6.7203 - accuracy: 0.0520\n",
            "Epoch 10/200\n",
            "1515/1515 [==============================] - 32s 21ms/step - loss: 6.5796 - accuracy: 0.0576\n",
            "Epoch 11/200\n",
            "1515/1515 [==============================] - 32s 21ms/step - loss: 6.4456 - accuracy: 0.0639\n",
            "Epoch 12/200\n",
            "1515/1515 [==============================] - 32s 21ms/step - loss: 6.3172 - accuracy: 0.0687\n",
            "Epoch 13/200\n",
            "1515/1515 [==============================] - 32s 21ms/step - loss: 6.1932 - accuracy: 0.0752\n",
            "Epoch 14/200\n",
            "1515/1515 [==============================] - 32s 21ms/step - loss: 6.0789 - accuracy: 0.0808\n",
            "Epoch 15/200\n",
            "1515/1515 [==============================] - 32s 21ms/step - loss: 5.9629 - accuracy: 0.0873\n",
            "Epoch 16/200\n",
            "1515/1515 [==============================] - 32s 21ms/step - loss: 5.8551 - accuracy: 0.0942\n",
            "Epoch 17/200\n",
            "1515/1515 [==============================] - 31s 21ms/step - loss: 5.7508 - accuracy: 0.1005\n",
            "Epoch 18/200\n",
            "1515/1515 [==============================] - 32s 21ms/step - loss: 5.6500 - accuracy: 0.1073\n",
            "Epoch 19/200\n",
            "1515/1515 [==============================] - 32s 21ms/step - loss: 5.5531 - accuracy: 0.1132\n",
            "Epoch 20/200\n",
            "1515/1515 [==============================] - 32s 21ms/step - loss: 5.4613 - accuracy: 0.1209\n",
            "Epoch 21/200\n",
            "1515/1515 [==============================] - 32s 21ms/step - loss: 5.3735 - accuracy: 0.1282\n",
            "Epoch 22/200\n",
            "1515/1515 [==============================] - 32s 21ms/step - loss: 5.2872 - accuracy: 0.1345\n",
            "Epoch 23/200\n",
            "1515/1515 [==============================] - 32s 21ms/step - loss: 5.2045 - accuracy: 0.1426\n",
            "Epoch 24/200\n",
            "1515/1515 [==============================] - 32s 21ms/step - loss: 5.1255 - accuracy: 0.1506\n",
            "Epoch 25/200\n",
            "1515/1515 [==============================] - 32s 21ms/step - loss: 5.0482 - accuracy: 0.1581\n",
            "Epoch 26/200\n",
            "1515/1515 [==============================] - 31s 21ms/step - loss: 4.9749 - accuracy: 0.1671\n",
            "Epoch 27/200\n",
            "1515/1515 [==============================] - 32s 21ms/step - loss: 4.9061 - accuracy: 0.1736\n",
            "Epoch 28/200\n",
            "1515/1515 [==============================] - 31s 21ms/step - loss: 4.8390 - accuracy: 0.1819\n",
            "Epoch 29/200\n",
            "1515/1515 [==============================] - 32s 21ms/step - loss: 4.7743 - accuracy: 0.1904\n",
            "Epoch 30/200\n",
            "1515/1515 [==============================] - 32s 21ms/step - loss: 4.7138 - accuracy: 0.1966\n",
            "Epoch 31/200\n",
            "1515/1515 [==============================] - 32s 21ms/step - loss: 4.6536 - accuracy: 0.2051\n",
            "Epoch 32/200\n",
            "1515/1515 [==============================] - 32s 21ms/step - loss: 4.5930 - accuracy: 0.2138\n",
            "Epoch 33/200\n",
            "1515/1515 [==============================] - 32s 21ms/step - loss: 4.5355 - accuracy: 0.2179\n",
            "Epoch 34/200\n",
            "1515/1515 [==============================] - 32s 21ms/step - loss: 4.4784 - accuracy: 0.2260\n",
            "Epoch 35/200\n",
            "1515/1515 [==============================] - 32s 21ms/step - loss: 4.4232 - accuracy: 0.2330\n",
            "Epoch 36/200\n",
            "1515/1515 [==============================] - 32s 21ms/step - loss: 4.3711 - accuracy: 0.2403\n",
            "Epoch 37/200\n",
            "1515/1515 [==============================] - 32s 21ms/step - loss: 4.3230 - accuracy: 0.2480\n",
            "Epoch 38/200\n",
            "1515/1515 [==============================] - 31s 20ms/step - loss: 4.2761 - accuracy: 0.2533\n",
            "Epoch 39/200\n",
            "1515/1515 [==============================] - 32s 21ms/step - loss: 4.2318 - accuracy: 0.2604\n",
            "Epoch 40/200\n",
            "1515/1515 [==============================] - 31s 20ms/step - loss: 4.1857 - accuracy: 0.2652\n",
            "Epoch 41/200\n",
            "1515/1515 [==============================] - 31s 21ms/step - loss: 4.1444 - accuracy: 0.2706\n",
            "Epoch 42/200\n",
            "1515/1515 [==============================] - 32s 21ms/step - loss: 4.1048 - accuracy: 0.2776\n",
            "Epoch 43/200\n",
            "1515/1515 [==============================] - 32s 21ms/step - loss: 4.0597 - accuracy: 0.2825\n",
            "Epoch 44/200\n",
            "1515/1515 [==============================] - 32s 21ms/step - loss: 4.0215 - accuracy: 0.2899\n",
            "Epoch 45/200\n",
            "1515/1515 [==============================] - 31s 21ms/step - loss: 3.9829 - accuracy: 0.2948\n",
            "Epoch 46/200\n",
            "1515/1515 [==============================] - 32s 21ms/step - loss: 3.9452 - accuracy: 0.2992\n",
            "Epoch 47/200\n",
            "1515/1515 [==============================] - 31s 20ms/step - loss: 3.9066 - accuracy: 0.3042\n",
            "Epoch 48/200\n",
            "1515/1515 [==============================] - 32s 21ms/step - loss: 3.8734 - accuracy: 0.3090\n",
            "Epoch 49/200\n",
            "1515/1515 [==============================] - 31s 21ms/step - loss: 3.8391 - accuracy: 0.3150\n",
            "Epoch 50/200\n",
            "1515/1515 [==============================] - 32s 21ms/step - loss: 3.8029 - accuracy: 0.3200\n",
            "Epoch 51/200\n",
            "1515/1515 [==============================] - 31s 21ms/step - loss: 3.7682 - accuracy: 0.3255\n",
            "Epoch 52/200\n",
            "1515/1515 [==============================] - 32s 21ms/step - loss: 3.7375 - accuracy: 0.3295\n",
            "Epoch 53/200\n",
            "1515/1515 [==============================] - 32s 21ms/step - loss: 3.7065 - accuracy: 0.3340\n",
            "Epoch 54/200\n",
            "1515/1515 [==============================] - 31s 21ms/step - loss: 3.6745 - accuracy: 0.3388\n",
            "Epoch 55/200\n",
            "1515/1515 [==============================] - 32s 21ms/step - loss: 3.6482 - accuracy: 0.3446\n",
            "Epoch 56/200\n",
            "1515/1515 [==============================] - 31s 21ms/step - loss: 3.6137 - accuracy: 0.3485\n",
            "Epoch 57/200\n",
            "1515/1515 [==============================] - 32s 21ms/step - loss: 3.5827 - accuracy: 0.3529\n",
            "Epoch 58/200\n",
            "1515/1515 [==============================] - 31s 20ms/step - loss: 3.5500 - accuracy: 0.3584\n",
            "Epoch 59/200\n",
            "1515/1515 [==============================] - 32s 21ms/step - loss: 3.5272 - accuracy: 0.3619\n",
            "Epoch 60/200\n",
            "1515/1515 [==============================] - 31s 20ms/step - loss: 3.4997 - accuracy: 0.3665\n",
            "Epoch 61/200\n",
            "1515/1515 [==============================] - 32s 21ms/step - loss: 3.4703 - accuracy: 0.3695\n",
            "Epoch 62/200\n",
            "1515/1515 [==============================] - 31s 20ms/step - loss: 3.4475 - accuracy: 0.3761\n",
            "Epoch 63/200\n",
            "1515/1515 [==============================] - 32s 21ms/step - loss: 3.4172 - accuracy: 0.3789\n",
            "Epoch 64/200\n",
            "1515/1515 [==============================] - 32s 21ms/step - loss: 3.3879 - accuracy: 0.3844\n",
            "Epoch 65/200\n",
            "1515/1515 [==============================] - 31s 20ms/step - loss: 3.3626 - accuracy: 0.3879\n",
            "Epoch 66/200\n",
            "1515/1515 [==============================] - 32s 21ms/step - loss: 3.3348 - accuracy: 0.3927\n",
            "Epoch 67/200\n",
            "1515/1515 [==============================] - 32s 21ms/step - loss: 3.3051 - accuracy: 0.3967\n",
            "Epoch 68/200\n",
            "1515/1515 [==============================] - 32s 21ms/step - loss: 3.2831 - accuracy: 0.3991\n",
            "Epoch 69/200\n",
            "1515/1515 [==============================] - 31s 20ms/step - loss: 3.2621 - accuracy: 0.4032\n",
            "Epoch 70/200\n",
            "1515/1515 [==============================] - 32s 21ms/step - loss: 3.2423 - accuracy: 0.4071\n",
            "Epoch 71/200\n",
            "1515/1515 [==============================] - 31s 20ms/step - loss: 3.2124 - accuracy: 0.4113\n",
            "Epoch 72/200\n",
            "1515/1515 [==============================] - 32s 21ms/step - loss: 3.1861 - accuracy: 0.4167\n",
            "Epoch 73/200\n",
            "1515/1515 [==============================] - 32s 21ms/step - loss: 3.1676 - accuracy: 0.4172\n",
            "Epoch 74/200\n",
            "1515/1515 [==============================] - 31s 20ms/step - loss: 3.1478 - accuracy: 0.4210\n",
            "Epoch 75/200\n",
            "1515/1515 [==============================] - 31s 21ms/step - loss: 3.1231 - accuracy: 0.4240\n",
            "Epoch 76/200\n",
            "1515/1515 [==============================] - 31s 20ms/step - loss: 3.1049 - accuracy: 0.4292\n",
            "Epoch 77/200\n",
            "1515/1515 [==============================] - 32s 21ms/step - loss: 3.0840 - accuracy: 0.4324\n",
            "Epoch 78/200\n",
            "1515/1515 [==============================] - 31s 20ms/step - loss: 3.0633 - accuracy: 0.4341\n",
            "Epoch 79/200\n",
            "1515/1515 [==============================] - 31s 21ms/step - loss: 3.0450 - accuracy: 0.4386\n",
            "Epoch 80/200\n",
            "1515/1515 [==============================] - 31s 21ms/step - loss: 3.0271 - accuracy: 0.4421\n",
            "Epoch 81/200\n",
            "1515/1515 [==============================] - 31s 21ms/step - loss: 3.0006 - accuracy: 0.4446\n",
            "Epoch 82/200\n",
            "1515/1515 [==============================] - 31s 20ms/step - loss: 2.9842 - accuracy: 0.4486\n",
            "Epoch 83/200\n",
            "1515/1515 [==============================] - 31s 21ms/step - loss: 2.9671 - accuracy: 0.4499\n",
            "Epoch 84/200\n",
            "1515/1515 [==============================] - 31s 20ms/step - loss: 2.9514 - accuracy: 0.4521\n",
            "Epoch 85/200\n",
            "1515/1515 [==============================] - 31s 21ms/step - loss: 2.9462 - accuracy: 0.4531\n",
            "Epoch 86/200\n",
            "1515/1515 [==============================] - 30s 20ms/step - loss: 2.9049 - accuracy: 0.4616\n",
            "Epoch 87/200\n",
            "1515/1515 [==============================] - 31s 21ms/step - loss: 2.8896 - accuracy: 0.4625\n",
            "Epoch 88/200\n",
            "1515/1515 [==============================] - 31s 21ms/step - loss: 2.8811 - accuracy: 0.4653\n",
            "Epoch 89/200\n",
            "1515/1515 [==============================] - 31s 20ms/step - loss: 2.8600 - accuracy: 0.4687\n",
            "Epoch 90/200\n",
            "1515/1515 [==============================] - 31s 21ms/step - loss: 2.8464 - accuracy: 0.4716\n",
            "Epoch 91/200\n",
            "1515/1515 [==============================] - 31s 20ms/step - loss: 2.8268 - accuracy: 0.4725\n",
            "Epoch 92/200\n",
            "1515/1515 [==============================] - 32s 21ms/step - loss: 2.8088 - accuracy: 0.4761\n",
            "Epoch 93/200\n",
            "1515/1515 [==============================] - 31s 20ms/step - loss: 2.7954 - accuracy: 0.4788\n",
            "Epoch 94/200\n",
            "1515/1515 [==============================] - 31s 21ms/step - loss: 2.7799 - accuracy: 0.4804\n",
            "Epoch 95/200\n",
            "1515/1515 [==============================] - 31s 20ms/step - loss: 2.7663 - accuracy: 0.4830\n",
            "Epoch 96/200\n",
            "1515/1515 [==============================] - 31s 21ms/step - loss: 2.8221 - accuracy: 0.4705\n",
            "Epoch 97/200\n",
            "1515/1515 [==============================] - 31s 20ms/step - loss: 2.8296 - accuracy: 0.4667\n",
            "Epoch 98/200\n",
            "1515/1515 [==============================] - 32s 21ms/step - loss: 2.7532 - accuracy: 0.4840\n",
            "Epoch 99/200\n",
            "1515/1515 [==============================] - 31s 20ms/step - loss: 2.7268 - accuracy: 0.4894\n",
            "Epoch 100/200\n",
            "1515/1515 [==============================] - 31s 21ms/step - loss: 2.7163 - accuracy: 0.4915\n",
            "Epoch 101/200\n",
            "1515/1515 [==============================] - 31s 20ms/step - loss: 2.7059 - accuracy: 0.4929\n",
            "Epoch 102/200\n",
            "1515/1515 [==============================] - 32s 21ms/step - loss: 2.6781 - accuracy: 0.4982\n",
            "Epoch 103/200\n",
            "1515/1515 [==============================] - 31s 20ms/step - loss: 2.6604 - accuracy: 0.5002\n",
            "Epoch 104/200\n",
            "1515/1515 [==============================] - 31s 21ms/step - loss: 2.6505 - accuracy: 0.5019\n",
            "Epoch 105/200\n",
            "1515/1515 [==============================] - 30s 20ms/step - loss: 2.6422 - accuracy: 0.5019\n",
            "Epoch 106/200\n",
            "1515/1515 [==============================] - 31s 21ms/step - loss: 2.6292 - accuracy: 0.5046\n",
            "Epoch 107/200\n",
            "1515/1515 [==============================] - 31s 20ms/step - loss: 2.6111 - accuracy: 0.5084\n",
            "Epoch 108/200\n",
            "1515/1515 [==============================] - 31s 20ms/step - loss: 2.5923 - accuracy: 0.5111\n",
            "Epoch 109/200\n",
            "1515/1515 [==============================] - 30s 20ms/step - loss: 2.5804 - accuracy: 0.5136\n",
            "Epoch 110/200\n",
            "1515/1515 [==============================] - 32s 21ms/step - loss: 2.5733 - accuracy: 0.5137\n",
            "Epoch 111/200\n",
            "1515/1515 [==============================] - 31s 20ms/step - loss: 2.5601 - accuracy: 0.5148\n",
            "Epoch 112/200\n",
            "1515/1515 [==============================] - 31s 20ms/step - loss: 2.5467 - accuracy: 0.5180\n",
            "Epoch 113/200\n",
            "1515/1515 [==============================] - 31s 21ms/step - loss: 2.5240 - accuracy: 0.5233\n",
            "Epoch 114/200\n",
            "1515/1515 [==============================] - 31s 20ms/step - loss: 2.5210 - accuracy: 0.5244\n",
            "Epoch 115/200\n",
            "1515/1515 [==============================] - 31s 20ms/step - loss: 2.5063 - accuracy: 0.5265\n",
            "Epoch 116/200\n",
            "1515/1515 [==============================] - 31s 20ms/step - loss: 2.4934 - accuracy: 0.5280\n",
            "Epoch 117/200\n",
            "1515/1515 [==============================] - 31s 20ms/step - loss: 2.4855 - accuracy: 0.5287\n",
            "Epoch 118/200\n",
            "1515/1515 [==============================] - 31s 20ms/step - loss: 2.4643 - accuracy: 0.5327\n",
            "Epoch 119/200\n",
            "1515/1515 [==============================] - 31s 21ms/step - loss: 2.4690 - accuracy: 0.5323\n",
            "Epoch 120/200\n",
            "1515/1515 [==============================] - 31s 20ms/step - loss: 2.4527 - accuracy: 0.5347\n",
            "Epoch 121/200\n",
            "1515/1515 [==============================] - 32s 21ms/step - loss: 2.4388 - accuracy: 0.5371\n",
            "Epoch 122/200\n",
            "1515/1515 [==============================] - 31s 20ms/step - loss: 2.4266 - accuracy: 0.5378\n",
            "Epoch 123/200\n",
            "1515/1515 [==============================] - 31s 21ms/step - loss: 2.4142 - accuracy: 0.5416\n",
            "Epoch 124/200\n",
            "1515/1515 [==============================] - 30s 20ms/step - loss: 2.4061 - accuracy: 0.5412\n",
            "Epoch 125/200\n",
            "1515/1515 [==============================] - 31s 21ms/step - loss: 2.4014 - accuracy: 0.5411\n",
            "Epoch 126/200\n",
            "1515/1515 [==============================] - 30s 20ms/step - loss: 2.3981 - accuracy: 0.5423\n",
            "Epoch 127/200\n",
            "1515/1515 [==============================] - 31s 20ms/step - loss: 2.3694 - accuracy: 0.5466\n",
            "Epoch 128/200\n",
            "1515/1515 [==============================] - 30s 20ms/step - loss: 2.3643 - accuracy: 0.5507\n",
            "Epoch 129/200\n",
            "1515/1515 [==============================] - 31s 20ms/step - loss: 2.3513 - accuracy: 0.5500\n",
            "Epoch 130/200\n",
            "1515/1515 [==============================] - 31s 20ms/step - loss: 2.3454 - accuracy: 0.5525\n",
            "Epoch 131/200\n",
            "1515/1515 [==============================] - 31s 21ms/step - loss: 2.3359 - accuracy: 0.5518\n",
            "Epoch 132/200\n",
            "1515/1515 [==============================] - 31s 20ms/step - loss: 2.3303 - accuracy: 0.5532\n",
            "Epoch 133/200\n",
            "1515/1515 [==============================] - 31s 21ms/step - loss: 2.3101 - accuracy: 0.5585\n",
            "Epoch 134/200\n",
            "1515/1515 [==============================] - 31s 20ms/step - loss: 2.3081 - accuracy: 0.5581\n",
            "Epoch 135/200\n",
            "1515/1515 [==============================] - 31s 21ms/step - loss: 2.3004 - accuracy: 0.5592\n",
            "Epoch 136/200\n",
            "1515/1515 [==============================] - 30s 20ms/step - loss: 2.4941 - accuracy: 0.5205\n",
            "Epoch 137/200\n",
            "1515/1515 [==============================] - 31s 21ms/step - loss: 2.2897 - accuracy: 0.5594\n",
            "Epoch 138/200\n",
            "1515/1515 [==============================] - 31s 20ms/step - loss: 2.2830 - accuracy: 0.5620\n",
            "Epoch 139/200\n",
            "1515/1515 [==============================] - 31s 20ms/step - loss: 2.2642 - accuracy: 0.5649\n",
            "Epoch 140/200\n",
            "1515/1515 [==============================] - 30s 20ms/step - loss: 2.2490 - accuracy: 0.5670\n",
            "Epoch 141/200\n",
            "1515/1515 [==============================] - 31s 21ms/step - loss: 2.2510 - accuracy: 0.5670\n",
            "Epoch 142/200\n",
            "1515/1515 [==============================] - 30s 20ms/step - loss: 2.2390 - accuracy: 0.5700\n",
            "Epoch 143/200\n",
            "1515/1515 [==============================] - 31s 21ms/step - loss: 2.2213 - accuracy: 0.5713\n",
            "Epoch 144/200\n",
            "1515/1515 [==============================] - 31s 20ms/step - loss: 2.2091 - accuracy: 0.5743\n",
            "Epoch 145/200\n",
            "1515/1515 [==============================] - 31s 21ms/step - loss: 2.2100 - accuracy: 0.5757\n",
            "Epoch 146/200\n",
            "1515/1515 [==============================] - 31s 20ms/step - loss: 2.2028 - accuracy: 0.5752\n",
            "Epoch 147/200\n",
            "1515/1515 [==============================] - 31s 21ms/step - loss: 2.1916 - accuracy: 0.5759\n",
            "Epoch 148/200\n",
            "1515/1515 [==============================] - 31s 20ms/step - loss: 2.1863 - accuracy: 0.5773\n",
            "Epoch 149/200\n",
            "1515/1515 [==============================] - 33s 22ms/step - loss: 2.1784 - accuracy: 0.5791\n",
            "Epoch 150/200\n",
            "1515/1515 [==============================] - 32s 21ms/step - loss: 2.1692 - accuracy: 0.5814\n",
            "Epoch 151/200\n",
            "1515/1515 [==============================] - 31s 20ms/step - loss: 2.1802 - accuracy: 0.5789\n",
            "Epoch 152/200\n",
            "1515/1515 [==============================] - 31s 21ms/step - loss: 2.1603 - accuracy: 0.5812\n",
            "Epoch 153/200\n",
            "1515/1515 [==============================] - 31s 20ms/step - loss: 2.1545 - accuracy: 0.5819\n",
            "Epoch 154/200\n",
            "1515/1515 [==============================] - 31s 21ms/step - loss: 2.1392 - accuracy: 0.5869\n",
            "Epoch 155/200\n",
            "1515/1515 [==============================] - 31s 20ms/step - loss: 2.1316 - accuracy: 0.5869\n",
            "Epoch 156/200\n",
            "1515/1515 [==============================] - 32s 21ms/step - loss: 2.1243 - accuracy: 0.5888\n",
            "Epoch 157/200\n",
            "1515/1515 [==============================] - 32s 21ms/step - loss: 2.1099 - accuracy: 0.5906\n",
            "Epoch 158/200\n",
            "1515/1515 [==============================] - 32s 21ms/step - loss: 2.1007 - accuracy: 0.5920\n",
            "Epoch 159/200\n",
            "1515/1515 [==============================] - 31s 21ms/step - loss: 2.1005 - accuracy: 0.5912\n",
            "Epoch 160/200\n",
            "1515/1515 [==============================] - 31s 21ms/step - loss: 2.0980 - accuracy: 0.5913\n",
            "Epoch 161/200\n",
            "1515/1515 [==============================] - 32s 21ms/step - loss: 2.0900 - accuracy: 0.5945\n",
            "Epoch 162/200\n",
            "1515/1515 [==============================] - 31s 21ms/step - loss: 2.0754 - accuracy: 0.5965\n",
            "Epoch 163/200\n",
            "1515/1515 [==============================] - 32s 21ms/step - loss: 2.0667 - accuracy: 0.5984\n",
            "Epoch 164/200\n",
            "1515/1515 [==============================] - 31s 21ms/step - loss: 2.0660 - accuracy: 0.5977\n",
            "Epoch 165/200\n",
            "1515/1515 [==============================] - 32s 21ms/step - loss: 2.0562 - accuracy: 0.5989\n",
            "Epoch 166/200\n",
            "1515/1515 [==============================] - 31s 20ms/step - loss: 2.0614 - accuracy: 0.5976\n",
            "Epoch 167/200\n",
            "1515/1515 [==============================] - 31s 21ms/step - loss: 2.0606 - accuracy: 0.5988\n",
            "Epoch 168/200\n",
            "1515/1515 [==============================] - 31s 20ms/step - loss: 2.0264 - accuracy: 0.6058\n",
            "Epoch 169/200\n",
            "1515/1515 [==============================] - 31s 21ms/step - loss: 2.0302 - accuracy: 0.6029\n",
            "Epoch 170/200\n",
            "1515/1515 [==============================] - 31s 20ms/step - loss: 2.0304 - accuracy: 0.6032\n",
            "Epoch 171/200\n",
            "1515/1515 [==============================] - 31s 21ms/step - loss: 2.0211 - accuracy: 0.6066\n",
            "Epoch 172/200\n",
            "1515/1515 [==============================] - 31s 20ms/step - loss: 2.0168 - accuracy: 0.6065\n",
            "Epoch 173/200\n",
            "1515/1515 [==============================] - 31s 21ms/step - loss: 2.0157 - accuracy: 0.6050\n",
            "Epoch 174/200\n",
            "1515/1515 [==============================] - 30s 20ms/step - loss: 2.0012 - accuracy: 0.6078\n",
            "Epoch 175/200\n",
            "1515/1515 [==============================] - 31s 21ms/step - loss: 1.9994 - accuracy: 0.6106\n",
            "Epoch 176/200\n",
            "1515/1515 [==============================] - 31s 20ms/step - loss: 1.9848 - accuracy: 0.6104\n",
            "Epoch 177/200\n",
            "1515/1515 [==============================] - 31s 21ms/step - loss: 1.9733 - accuracy: 0.6122\n",
            "Epoch 178/200\n",
            "1515/1515 [==============================] - 31s 20ms/step - loss: 1.9775 - accuracy: 0.6131\n",
            "Epoch 179/200\n",
            "1515/1515 [==============================] - 31s 20ms/step - loss: 1.9670 - accuracy: 0.6132\n",
            "Epoch 180/200\n",
            "1515/1515 [==============================] - 31s 21ms/step - loss: 1.9780 - accuracy: 0.6103\n",
            "Epoch 181/200\n",
            "1515/1515 [==============================] - 31s 21ms/step - loss: 1.9611 - accuracy: 0.6148\n",
            "Epoch 182/200\n",
            "1515/1515 [==============================] - 31s 21ms/step - loss: 1.9430 - accuracy: 0.6180\n",
            "Epoch 183/200\n",
            "1515/1515 [==============================] - 31s 20ms/step - loss: 1.9459 - accuracy: 0.6166\n",
            "Epoch 184/200\n",
            "1515/1515 [==============================] - 32s 21ms/step - loss: 1.9385 - accuracy: 0.6196\n",
            "Epoch 185/200\n",
            "1515/1515 [==============================] - 31s 20ms/step - loss: 1.9253 - accuracy: 0.6226\n",
            "Epoch 186/200\n",
            "1515/1515 [==============================] - 31s 21ms/step - loss: 1.9253 - accuracy: 0.6212\n",
            "Epoch 187/200\n",
            "1515/1515 [==============================] - 30s 20ms/step - loss: 1.9238 - accuracy: 0.6210\n",
            "Epoch 188/200\n",
            "1515/1515 [==============================] - 31s 21ms/step - loss: 1.9183 - accuracy: 0.6228\n",
            "Epoch 189/200\n",
            "1515/1515 [==============================] - 31s 20ms/step - loss: 1.9208 - accuracy: 0.6224\n",
            "Epoch 190/200\n",
            "1515/1515 [==============================] - 32s 21ms/step - loss: 1.9356 - accuracy: 0.6182\n",
            "Epoch 191/200\n",
            "1515/1515 [==============================] - 31s 21ms/step - loss: 1.8988 - accuracy: 0.6271\n",
            "Epoch 192/200\n",
            "1515/1515 [==============================] - 31s 21ms/step - loss: 1.8904 - accuracy: 0.6269\n",
            "Epoch 193/200\n",
            "1515/1515 [==============================] - 32s 21ms/step - loss: 1.8957 - accuracy: 0.6256\n",
            "Epoch 194/200\n",
            "1515/1515 [==============================] - 31s 20ms/step - loss: 1.8814 - accuracy: 0.6305\n",
            "Epoch 195/200\n",
            "1515/1515 [==============================] - 31s 21ms/step - loss: 1.8896 - accuracy: 0.6269\n",
            "Epoch 196/200\n",
            "1515/1515 [==============================] - 31s 20ms/step - loss: 1.8637 - accuracy: 0.6320\n",
            "Epoch 197/200\n",
            "1515/1515 [==============================] - 31s 21ms/step - loss: 1.8721 - accuracy: 0.6300\n",
            "Epoch 198/200\n",
            "1515/1515 [==============================] - 31s 20ms/step - loss: 1.8760 - accuracy: 0.6294\n",
            "Epoch 199/200\n",
            "1515/1515 [==============================] - 31s 20ms/step - loss: 1.8688 - accuracy: 0.6324\n",
            "Epoch 200/200\n",
            "1515/1515 [==============================] - 31s 21ms/step - loss: 1.8737 - accuracy: 0.6309\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(xs, ys, epochs=200, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "id": "597b2a55",
      "metadata": {
        "id": "597b2a55"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#predicting the next word using an initial sentence\n",
        "input_phrase = \"the weather today is sunny\"\n",
        "next_words = 5\n",
        "\n",
        "for _ in range(next_words):\n",
        "    #converting our input_phrase to tokens and excluding the out of vcabulary words\n",
        "    token_list = tokenizer.texts_to_sequences([input_phrase])[0]\n",
        "    #padding the input_phrase\n",
        "    token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "    #predicting the token of the next word using our trained model\n",
        "    predicted = model.predict(token_list, verbose=0)\n",
        "    output_word = \"\" #initialising output word as blank at the beginning\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "        if index == predicted:\n",
        "            #converting the token back to the corresponding word and\n",
        "            #storing it in the output_word\n",
        "            output_word = word\n",
        "            break\n",
        "    input_phrase += \" \" + output_word\n",
        "print(input_phrase)"
      ],
      "metadata": {
        "id": "VEZS4Bt6s6n_"
      },
      "id": "VEZS4Bt6s6n_"
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "id": "95c5d5c3",
      "metadata": {
        "id": "95c5d5c3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c24d9e6c-339b-4ac8-d107-a8586464b188"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected sentence from the input data:\n",
            "google translate invented language help translate effectively\n",
            "\n",
            "Selected sentence:\n",
            "google translate invented language help translate effectively\n",
            "\n",
            "Generated sentences:\n",
            "google translate invented language help translate effectively taking invented fully types profile even people enjoy web experience theyre right tech difficult lot hands providers however shes able\n",
            "google translate invented language help translate effectively taking invented fully types profile even people enjoy web experience theyre right tech difficult lot hands providers however shes able learn let people better exactly evaluate let gives us know set information time table features ram modification resist referred method\n",
            "google translate invented language help translate effectively taking invented fully types profile even people enjoy web experience theyre right tech difficult lot hands providers however shes able learn let people better exactly evaluate let gives us know set information time table features ram modification resist referred method ratings index allows us action component explore one action used intuitions think situations process generic supervised animals predict cognition machine\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "\n",
        "# Preprocess text column\n",
        "corpus = []\n",
        "for text in df['text']:\n",
        "    preprocessed_sentences = preprocess_text(text)\n",
        "    corpus.extend(preprocessed_sentences)\n",
        "\n",
        "# Randomly select a sentence from the corpus\n",
        "selected_sentence = random.choice(corpus)\n",
        "print(\"Selected sentence from the input data:\")\n",
        "print(selected_sentence)\n",
        "\n",
        "# Set the input_phrase as the selected sentence\n",
        "input_phrase = selected_sentence\n",
        "\n",
        "# Define the number of sentences to generate\n",
        "num_sentences = 3\n",
        "\n",
        "# Define the maximum length of the generated sentences\n",
        "max_sentence_length = 20\n",
        "\n",
        "# List to store the generated sentences\n",
        "generated_sentences = []\n",
        "\n",
        "# Generate additional sentences of similar context\n",
        "for _ in range(num_sentences):\n",
        "    generated_sentence = input_phrase.split()  # Initialize with the input phrase\n",
        "\n",
        "    # Generate the next word until reaching maximum length or end-of-sentence token\n",
        "    for _ in range(max_sentence_length):\n",
        "        # Tokenize the input_phrase and pad it\n",
        "        token_list = tokenizer.texts_to_sequences([input_phrase])[0]\n",
        "        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "\n",
        "        # Predict the next word using the trained model\n",
        "        predicted = model.predict(token_list, verbose=0)\n",
        "\n",
        "        # Sample the next word from the predicted probability distribution\n",
        "        predicted_word_index = np.random.choice(len(predicted[0]), p=predicted[0])\n",
        "\n",
        "        # Convert the token back to the corresponding word\n",
        "        output_word = \"\"\n",
        "        for word, index in tokenizer.word_index.items():\n",
        "            if index == predicted_word_index:\n",
        "                output_word = word\n",
        "                break\n",
        "\n",
        "        # Append the predicted word to the generated sentence\n",
        "        generated_sentence.append(output_word)\n",
        "\n",
        "        # If the predicted word is an end-of-sentence token, stop generating\n",
        "        if output_word == '.' or output_word == '?' or output_word == '!':\n",
        "            break\n",
        "\n",
        "        # Update the input_phrase with the new generated sentence\n",
        "        input_phrase = ' '.join(generated_sentence)\n",
        "\n",
        "    # Add the generated sentence to the list\n",
        "    generated_sentences.append(' '.join(generated_sentence))\n",
        "\n",
        "# Print the selected sentence and the final generated sentences\n",
        "print(\"\\nSelected sentence:\")\n",
        "print(selected_sentence)\n",
        "print(\"\\nGenerated sentences:\")\n",
        "for sentence in generated_sentences:\n",
        "    print(sentence)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "5rO4ZSKLs5hh"
      },
      "id": "5rO4ZSKLs5hh"
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "id": "d8dd186a",
      "metadata": {
        "id": "d8dd186a"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "id": "1f5fbf36",
      "metadata": {
        "id": "1f5fbf36"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "id": "989cf69b",
      "metadata": {
        "id": "989cf69b"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "id": "4b459e43",
      "metadata": {
        "id": "4b459e43"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "id": "745584fe",
      "metadata": {
        "id": "745584fe"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "id": "07feeadb",
      "metadata": {
        "id": "07feeadb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "id": "3fb44475",
      "metadata": {
        "id": "3fb44475"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}